{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Welcome to the first assignment of the course Probability and Statistics for Machine Learning and Data Science! The last course in the Math for Machine Learning and Data Science specialization! In this assignment you will implement the Naive Bayes algorithm for a spam detection problem, as you saw in the lectures. The Sections 1 - 3 provide useful context on the problem. In Section 4 you will write functions to actually implement the algorithm. Section 5 includes some interesting ungraded extensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- [ 1 - Introduction](#1)\n",
    "- [ 2 - Necessary imports](#2)\n",
    "- [ 3 - The Dataset](#3)\n",
    "  - [ 3.1 Loading and Exploring the Dataset](#3.1)\n",
    "  - [ 3.2 Preprocessing the dataset](#3.2)\n",
    "  - [ 3.3 Preprocessing the text](#3.3)\n",
    "  - [ 3.4 Splitting into train/test](#3.4)\n",
    "- [ 4 - Implementing the Naive Bayes Algorithm](#4)\n",
    "  - [ 4.1 Computing $P(\\text{email} \\mid \\text{spam})$ and $P(\\text{email} \\mid \\text{ham})$](#4.1)\n",
    "  - [ 4.2 Computing $P(\\text{spam})$ and $P(\\text{ham})$](#4.2)\n",
    "  - [ 4.3 Putting all together](#4.3)\n",
    "    - [ Exercise 1](#ex01)\n",
    "    - [ Exercise 2](#ex02)\n",
    "    - [ Exercise 3](#ex03)\n",
    "    - [ Exercise 4](#ex04)\n",
    "  - [ 4.4 Model performance](#4.4)\n",
    "- [ 5 - Appendix (Section NOT graded)](#5)\n",
    "  - [ 5.1 Hidden problem in the Naive Bayes model.](#5.1)\n",
    "  - [ 5.2 Enhancing model performance: Practical implementation with Naive Bayes](#5.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - Introduction\n",
    "\n",
    "The Naive Bayes algorithm stands as a cornerstone in Machine Learning and Data Science, leveraging Bayes' Theorem with the goal of determining whether a data point belongs to a specific class. The algorithm makes a \"naive assumption\" that each feature is independent of the others. This assumption almost certainly isn't true of your data, but making it leads to a significantly easier algorithm to implement, and as you'll see can lead to impressively useful results. It's important to note that Naive Bayes is a supervised algorithm, meaning it requires data that's already labeled to function effectively. In the example you're about to see, that means it requires that a collection of emails have already been marked as \"spam\" or \"ham\" in order to train the algorithm.\n",
    "\n",
    "### Naive Bayes for Spam Detection\n",
    "\n",
    "This assignment focuses on a binary classification problem: distinguishing between spam and non-spam emails, colloquially referred to as \"ham.\" For the purpose of this task, spam emails will be labeled as $1$, and non-spam (ham) emails as $0$.\n",
    "\n",
    "The probability of interest for a given email is denoted as:\n",
    "\n",
    "$$ P(\\text{spam} \\mid \\text{email}) $$\n",
    "\n",
    "The higher this probability, the more likely the email is to be classified as spam. Bayes' Theorem, which you saw in the lectures, is used in the calculation in the following way:\n",
    "\n",
    "$$ P(\\text{spam} \\mid \\text{email}) = \\frac{P(\\text{email} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{email})} $$\n",
    "\n",
    "Here's a breakdown of the terms:\n",
    "\n",
    "- $ P(\\text{spam}) $: Probability of a randomly selected email being spam, equivalent to the proportion of spam emails in the dataset.\n",
    "- $ P(\\text{email} \\mid \\text{spam}) $: Probability of a specific email occurring given that it is known to be spam.\n",
    "- $ P(\\text{email}) $: Overall probability of the email occurring.\n",
    "\n",
    "An interesting early \"shortcut\" you can take in this approach is just ignore the $ P(\\text{email}) $ term. The goal of this calculation will be to compare the probability an email is spam to the probability is ham. Here's the expression for both $ P(\\text{spam} \\mid \\text{email}) $ and $ P(\\text{ham} \\mid \\text{email}) $:\n",
    "\n",
    "$$ P(\\text{spam} \\mid \\text{email}) = \\frac{P(\\text{email} \\mid \\text{spam}) \\cdot P(\\text{spam})}{P(\\text{email})} $$\n",
    "\n",
    "$$ P(\\text{ham} \\mid \\text{email}) = \\frac{P(\\text{email} \\mid \\text{ham}) \\cdot P(\\text{ham})}{P(\\text{email})} $$\n",
    "\n",
    "Since $ P(\\text{email}) > 0 $ and it appears in both expressions, comparing the two probabilities only requires evaluating the numerators and you can ignore this denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - Necessary imports\n",
    "\n",
    "This next codeblock will import all necessary libraries and functions you will need in the assignment as well as unit tests that will provide feedback as you work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w1_unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - The Dataset\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Loading and Exploring the Dataset\n",
    "\n",
    "The following code block will load the dataset into memory. You will utilize the [Pandas Library](https://pandas.pydata.org/docs/index.html) to read it as a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html#pandas.DataFrame), its primary object. However, **no need to worry** if you are are still getting familiar with Pandas. It will be loaded to illustrate the data structure, and you will end up with NumPy arrays for your actual work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                                    text  spam\n",
       "0     Subject: naturally irresistible your corporate...     1\n",
       "1     Subject: the stock trading gunslinger  fanny i...     1\n",
       "2     Subject: unbelievable new homes made easy  im ...     1\n",
       "3     Subject: 4 color printing special  request add...     1\n",
       "4     Subject: do not have money , get software cds ...     1\n",
       "...                                                 ...   ...\n",
       "5723  Subject: re : research and development charges...     0\n",
       "5724  Subject: re : receipts from visit  jim ,  than...     0\n",
       "5725  Subject: re : enron case study update  wow ! a...     0\n",
       "5726  Subject: re : interest  david ,  please , call...     0\n",
       "5727  Subject: news : aurora 5 . 2 update  aurora ve...     0\n",
       "\n",
       "[5728 rows x 2 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_emails = pd.read_csv('emails.csv')\n",
    "dataframe_emails.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the dataset a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails: 5728\n",
      "Proportion of spam emails: 0.2388\n",
      "Proportion of ham emails: 0.7612\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of emails: {len(dataframe_emails)}\")\n",
    "print(f\"Proportion of spam emails: {dataframe_emails.spam.sum()/len(dataframe_emails):.4f}\")\n",
    "print(f\"Proportion of ham emails: {1-dataframe_emails.spam.sum()/len(dataframe_emails):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this dataset is **unbalanced**. There are more than twice as many ham emails as spam emails in it! This is useful context to know in any data analysis project and may affect how some machine learning algorithms run, including Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "### 3.2 Preprocessing the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame has two columns. The one called `text` has the email's contents and the second one, called `spam` has a numerical variable telling whether the email is a spam or not. Remember that $1$ means spam and $0$ means ham (not spam). This next function will complete a couple of important pre-processing steps:\n",
    "\n",
    "* Note that every email starts with `Subject:`. This function will remove this word from the front of every email.\n",
    "* It will randomly shuffle the dataset. Right now all the spam emails are at the top of the data set followed by the ham emails. You need a shuffled dataset to properly split them between train and test dataset.\n",
    "\n",
    "Don't worry if you don't understand all the Python in this function, but it's included here to remind you that usually you need to explore and pre-process your data before jumping right into analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_emails(df):\n",
    "    df = df.sample(frac = 1, ignore_index = True, random_state = 42)\n",
    "    X = df.text.apply(lambda x: x[9:]).to_numpy()\n",
    "    Y = df.spam.to_numpy()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = preprocess_emails(dataframe_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first $5$ emails:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re : energy derivatives conference - may 29 , toronto  good morning amy :  vince kaminski will need the following :  an lcd projector to hook up to a lap tap for his presentation  he will have dinner with the conference organizers and speakers on the 29 th .  he will need 2 nights ( the 28 th and the 29 th ) hotel reservations .  he will send you an abstract shortly .  thanks and have a great day !  shirley crenshaw  713 - 853 - 5290  amy aldous on 03 / 31 / 2000 10 : 50 : 11 am  to : shirley . crenshaw @ enron . com  cc :  subject : re : energy derivatives conference - may 29 , toronto  ms . crenshaw ,  thank you for sending the bio so quickly . it \\' s exactly what i was looking  for .  we are planning to compile the conference speakers \\' papers for distribution  to the participants . while i will not need dr . kaminski \\' s contribution for  several weeks , an abstract of his presentation as soon as possible would be  very useful to the conference organizers .  i will also need the following information :  - dr . kaminski \\' s audio / video equipment requirements for his presentation  - will he be joining the conference organizers and speakers for dinner on  may 29 ?  - which nights will he be staying in toronto ? i will reserve a room at the  conference hotel  - any dietary restrictions or special requests  your help is much appreciated .  best wishes ,  amy  at 11 : 50 am 3 / 30 / 00 - 0600 , you wrote :  >  > amy :  >  > attached please find a short \" bio \" for dr . kaminski . please let me know  > if i can help further .  >  >  > ( see attached file : vincent kaminski bio . doc )  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *  amy aldous , conference co - ordinator  centre for advanced studies in finance  university of waterloo  waterloo , on n 2 l 3 gl  tel : ( 519 ) 888 - 4567 ext . 5728  fax : ( 519 ) 888 - 7562  email : aaldous @ uwaterloo . ca  * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *'\n",
      " 'financial maths course , part 2  vince ,  just in case , here is a draft copy of the event for you to refer to .  paul  - finmathmail . doc'\n",
      " 're : bullet points  please respond to hi vince ,  thanks for the bullets . regarding power 2001 , it certainly does promise to  be a very interesting event .  have a great week ,  paul  - - - - - original message - - - - -  from : vince . j . kaminski @ enron . com [ mailto : vince . j . kaminski @ enron . com ]  sent : monday , april 09 , 2001 9 : 11 am  to : pbristow @ riskwaters . com  cc : vince . j . kaminski @ enron . com ; vkaminski @ aol . com  subject : bullet points  paul ,  i am sending you modified bullet points . the modifications are in red .  apologies for a delay in responding to your messages .  by the way , power 2001 gets only more and more interesting every day .  vince  ( see attached file : financial maths draft . doc )'\n",
      " 're : enron default swaps  darrell ,  i am sending you 2 technical notes on enron default swaps : i hope that they  will  be useful . i shall read the articles on weekend . i am curious if you  find these explanations satisfactory .  we are very slow in preparing a number of technical documents  for you for model reviews . we still hope you will be able  to find some time to review our credit models ( for our london  credit trading ) and var and option pricing related models .  also , please check your invoices . i still think we owe you money .  vince  darrell duffie on 03 / 28 / 2001 08 : 07 : 38 am  to : vince j kaminski  cc :  subject : re : enron default swaps  vince : according to a bank of america  publication , your ( enron ) default swap spreads  are consistently trading about 80  basis points wider than your asset swaps .  any idea of what is going on here ?  thanks for any guidance , darrell  darrell duffie  mail gsb stanford ca 94305 - 5015 usa  phone 650 723 1976  fax 650 725 7979  email duffie @ stanford . edu  web http : / / www . stanford . edu / ~ duffie / '\n",
      " 're : power question  steve ,  elena chilkina can give you historical data .  historical fwd curves can be obtained from paulo  or alex , among others . of course , our internal forward curves  represent a very sensitive information .  vince  steven leppard  10 / 13 / 2000 10 : 34 am  to : vince j kaminski / hou / ect @ ect  cc : didier magne / lon / ect @ ect  subject : power question  hi vince  who should i contact for power queries now grant has gone ? a colleague here  in london ( didier magne ) is giving a talk on power / gas arbitrage , and the  consequent convergence of these markets .  do you have any presentations on this area , or illustrative figures on the  increase in power / gas correlation ?  many thanks ,  steve']\n"
     ]
    }
   ],
   "source": [
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the first $5$ labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numpy array `X` is an array of strings, so each element in this array is an email and the same index in this array is the index in `Y` telling whether the email is spam or not. **Try changing the value in `email_index`** to see the text of various emails and whether they're spam or not. Remember that 0 means ham and 1 means spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email index 30: karthik rajan - interview schedule  attached you will find the interview packet for the above - referenced person .  the interview will happen friday , march 30 , 2001 . please print all three  documents for your hard copies . if you have any questions , or conflicts of  schedule , please do not hesitate to contact me .  sasha divelbiss  58714\n",
      "\n",
      "\n",
      "Class: 0\n"
     ]
    }
   ],
   "source": [
    "email_index = 30\n",
    "print(f\"Email index {email_index}: {X[email_index]}\\n\\n\")\n",
    "print(f\"Class: {Y[email_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "### 3.3 Preprocessing the text\n",
    "\n",
    "This section is not covered in the lectures and there is no graded function in it. However, it is important when dealing with text and learning about this will for sure help your path in Machine Learning and Data Science!\n",
    "\n",
    "In text, usually there are some words that don't provide much information about what the text is saying, such as prepositions, pronouns and so on. These are called **stopwords**. Since they are very common in every text, they hardly will store any meaningful information for our task. The idea is to remove all these stopwords and punctuation, so in the end you will have a simpler set of words to deal with. This is what the next function will do.\n",
    "\n",
    "Another step is the emails **tokenization**. To tokenize is to split the email into **tokens**, which are essentially the words in it. As a result, for each email, the final result will be a numpy array consisting of every word in the email without stopwords and punctuation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(X):\n",
    "    \"\"\"\n",
    "    Preprocesses a collection of text data by removing stopwords and punctuation.\n",
    "\n",
    "    Parameters:\n",
    "    - X (str or array-like): The input text data to be processed. If a single string is provided,\n",
    "      it will be converted into a one-element numpy array.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.array: An array of preprocessed text data, where each element represents a document\n",
    "      with stopwords and punctuation removed.\n",
    "\n",
    "    Note:\n",
    "    - The function uses the Natural Language Toolkit (nltk) library for tokenization and stopword removal.\n",
    "    - If the input is a single string, it is converted into a one-element numpy array.\n",
    "    \"\"\"\n",
    "    # Make a set with the stopwords and punctuation\n",
    "    stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "    # The next lines will handle the case where a single email is passed instead of an array of emails.\n",
    "    if isinstance(X, str):\n",
    "        X = np.array([X])\n",
    "\n",
    "    # The result will be stored in a list\n",
    "    X_preprocessed = []\n",
    "\n",
    "    for i, email in enumerate(X):\n",
    "        email = np.array([i.lower() for i in word_tokenize(email) if i.lower() not in stop]).astype(X.dtype)\n",
    "        X_preprocessed.append(email)\n",
    "        \n",
    "    if len(X) == 1:\n",
    "        return X_preprocessed[0]\n",
    "    return X_preprocessed\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function may take a few seconds to run. Usually less than 1 minute.\n",
    "X_treated = preprocess_text(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the pre-processing, the text of each email has been turned into a numpy array with all the stop words below. The example here shows how a randomly selected `email_index` value (in this case 989) looks before and after this processing step. Feel free to try out different values to see the results of this step on different emails. This cleaned up array of words for each email will be what is actually used by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email before preprocessing: marketing for your espeak session  vince :  thanks for your time earlier this week ; i ' m looking forward to your espeak  event .  sarah and i met with our etv contact yesterday , and we will be able to put a  bulleted list on the elevator screens to advertise your espeak . please let  me know what you would like us to post for you , and we will do the rest !  we also have plans to market specifically to the trader community here at  enron , so you should get a high participation rate , especially from those  groups .  thanks , again .  - er\n",
      "Email after preprocessing: ['marketing' 'espeak' 'session' 'vince' 'thanks' 'time' 'earlier' 'week'\n",
      " 'looking' 'forward' 'espeak' 'event' 'sarah' 'met' 'etv' 'contact'\n",
      " 'yesterday' 'able' 'put' 'bulleted' 'list' 'elevator' 'screens'\n",
      " 'advertise' 'espeak' 'please' 'let' 'know' 'would' 'like' 'us' 'post'\n",
      " 'rest' 'also' 'plans' 'market' 'specifically' 'trader' 'community'\n",
      " 'enron' 'get' 'high' 'participation' 'rate' 'especially' 'groups'\n",
      " 'thanks' 'er']\n"
     ]
    }
   ],
   "source": [
    "email_index = 989\n",
    "print(f\"Email before preprocessing: {X[email_index]}\")\n",
    "print(f\"Email after preprocessing: {X_treated[email_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.4\"></a>\n",
    "### 3.4 Splitting into train/test\n",
    "\n",
    "Now let's split our dataset into train and test sets. You will work with a proportion of 80/20, i.e., 80% of the data will be used for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(0.80*len(X_treated)) # 80% of the samples will be used to train.\n",
    "\n",
    "X_train = X_treated[:TRAIN_SIZE]\n",
    "Y_train = Y[:TRAIN_SIZE]\n",
    "X_test = X_treated[TRAIN_SIZE:]\n",
    "Y_test = Y[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that there about 24% of the emails are spam. It is important to check if this proportion remains roughly the same in the train and test datasets, otherwise you may build a biased algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of spam in train dataset: 0.2431\n",
      "Proportion of spam in test dataset: 0.2216\n"
     ]
    }
   ],
   "source": [
    "print(f\"Proportion of spam in train dataset: {sum(Y_train == 1)/len(Y_train):.4f}\")\n",
    "print(f\"Proportion of spam in test dataset: {sum(Y_test == 1)/len(Y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are not equal, but they are very close, so it is fine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - Implementing the Naive Bayes Algorithm\n",
    "\n",
    "Remember your task: Compare $P(\\text{spam} \\mid \\text{email})$ and $P(\\text{ham} \\mid \\text{email})$ to decide which one is greater. It is sufficient to compute only $P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})$ and $P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$ to make the comparison.\n",
    "\n",
    "<a name=\"4.1\"></a>\n",
    "### 4.1 Computing $P(\\text{email} \\mid \\text{spam})$ and $P(\\text{email} \\mid \\text{ham})$\n",
    "\n",
    "Both cases work identically, so let's start on the spam case.\n",
    "\n",
    "Each email is a list of words. Your goal is to calculate how likely you are to see this list of words, given the email is spam. The way you'll do that is to apply the product rule. Representing an email as $\\text{email} = \\{\\text{word}_1, \\text{word}_2, \\ldots, \\text{word}_n \\}$, the computation is:\n",
    "\n",
    "$$P(\\text{email} \\mid \\text{spam}) = P(\\text{word}_1 \\mid \\text{spam}) \\cdot P(\\text{word}_2 \\mid \\text{spam}) \\cdots P(\\text{word}_n \\mid \\text{spam})$$\n",
    "\n",
    "This is where you make the **naive assumption** that leads to the name \"Naive Bayes\"! You will assume that each word's probability of appearing in an email is independent of each other word's probability. This assumption, of course, is false. Emails that contain the word \"party\" are probably more likely to include the word \"invitation\". Emails that contain the word \"prize\" are probably more likely to include the word \"congratulations\". By making a false assumption that these probabilities are independent, however, you gain the ability to apply the product rule. Rather than accounting for a complex set of conditional probabilities between words, you can simply assume independence and multiply a fairly simple set of conditional probabilities as shown in the expression above. Naive Bayes is built on an inaccurate assumption about your data, but as you'll see, it often yields impressive results!\n",
    "\n",
    "Here's how you'd actually calculate the probability of $\\text{word}_1$ appearing in an email, given it's spam:\n",
    "\n",
    "$$P(\\text{word}_1 \\mid \\text{spam}) = \\frac{\\text{\\# spam emails with } \\text{word}_1}{\\text{\\# spam emails}}$$\n",
    "\n",
    "Where the symbol \\# means the number of elements, i.e., $\\text{\\# spam emails with } \\text{word}_1$ means the amount of spam emails with $\\text{word}_1$. \n",
    "\n",
    "This is actually a really simple calculation. Count up how many spam emails contain $\\text{word}_1$ and divide by the total number of spam emails. Iterate through every word in the dataset and repeat the process, and you're ready to calculate the overall probability of seeing any given email, given it is spam or ham. With this in mind, **your first task will be to create a dictionary named `word_frequency` to store the frequency with with every word in the dataset appears in ham and spam emails**\n",
    "\n",
    "#### 4.1.1 Handling 0 in the Product\n",
    "\n",
    "Encountering a word that only appears in spam emails or never appears in a spam email may result in $P(\\text{word} \\mid \\text{spam}) = 0$ (or the ham analog), leading to the entire product being $0$. This scenario is undesirable as a single word could make the entire probability $0$. To mitigate this, you will **start by counting spam/ham appearances for every word from 1**. By artificially assuming that there is at least one spam and one ham email with every word, you eliminate the possibility of $0$ appearing in the computations.\n",
    "\n",
    "<a name=\"4.2\"></a>\n",
    "### 4.2 Computing $P(\\text{spam})$ and $P(\\text{ham})$\n",
    "\n",
    "When using Bayes Theorem, you'll also need to include the overall probability of seeing ham and spam emails. This computation is fairly easy since they are just the proportion of spam and ham emails in the dataset. \n",
    "\n",
    "$$P(\\text{spam}) = \\frac{\\text{\\# spam emails}}{\\text{\\# total emails}}$$\n",
    "$$P(\\text{ham}) = \\frac{\\text{\\# ham emails}}{\\text{\\# total emails}}$$\n",
    "\n",
    "<a name=\"4.3\"></a>\n",
    "### 4.3 Putting all together\n",
    "\n",
    "To calculate the probability an email is spam or ham, you'll just need to multiply the terms you've already calculated and compare which one is bigger.\n",
    "\n",
    "- $P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})$\n",
    "- $P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "Your task is to implement the function that generates a dictionary, recording the frequency with which each word in the dataset appears as spam (1) or ham (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def get_word_frequency(X,Y):\n",
    "    \"\"\"\n",
    "    Calculate the frequency of each word in a set of emails categorized as spam (1) or not spam (0).\n",
    "\n",
    "    Parameters:\n",
    "    - X (numpy.array): Array of emails, where each email is represented as a list of words.\n",
    "    - Y (numpy.array): Array of labels corresponding to each email in X. 1 indicates spam, 0 indicates ham.\n",
    "\n",
    "    Returns:\n",
    "    - word_dict (dict): A dictionary where keys are unique words found in the emails, and values\n",
    "      are dictionaries containing the frequency of each word for spam (1) and not spam (0) emails.\n",
    "    \"\"\"\n",
    "    # Creates an empty dictionary\n",
    "    word_dict = {}\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    num_emails = len(X)\n",
    "\n",
    "    # Iterates over every processed email and its label\n",
    "    for i in range(num_emails):\n",
    "        # Get the i-th email\n",
    "        email = X[i] \n",
    "        # Get the i-th label. This indicates whether the email is spam or not. 1 = None\n",
    "        # The variable name cls is an abbreviation for class, a reserved word in Python.\n",
    "        cls = Y[i] \n",
    "        # To avoid counting the same word twice in an email, remove duplicates by casting the email as a set\n",
    "        email = set(email) \n",
    "        # Iterates over every distinct word in the email\n",
    "        for word in email:\n",
    "            # If the word is not already in the dictionary, manually add it. Remember that you will start every word count as 1 both in spam and ham\n",
    "            if word not in word_dict.keys():\n",
    "                word_dict[word] = {'spam': 1, 'ham': 1}\n",
    "            # Add one occurrence for that specific word in the key ham if cls == 0 and spam if cls == 1. \n",
    "            if cls == 0:    \n",
    "                word_dict[word][\"ham\"] += 1\n",
    "            if cls == 1:\n",
    "                word_dict[word][\"spam\"] += 1\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'river': {'spam': 2, 'ham': 3}, 'going': {'spam': 2, 'ham': 1}, 'like': {'spam': 2, 'ham': 1}, 'deep': {'spam': 1, 'ham': 2}, 'love': {'spam': 1, 'ham': 2}, 'hate': {'spam': 1, 'ham': 2}}\n"
     ]
    }
   ],
   "source": [
    "test_output = get_word_frequency([['like','going','river'], ['love', 'deep', 'river'], ['hate','river']], [1,0,0])\n",
    "print(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ (the output order may vary, what is important is the value for each word)\n",
    "\n",
    "```Python\n",
    "{'going': {'spam': 2, 'ham': 1}, 'river': {'spam': 2, 'ham': 3}, 'like': {'spam': 2, 'ham': 1}, 'deep': {'spam': 1, 'ham': 2}, 'love': {'spam': 1, 'ham': 2}, 'hate': {'spam': 1, 'ham': 2}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will test your function. Don't worry, you are not being graded yet. This will just ensure your function is working properly. If the unit test fails, you will get feedback so you can review your function before moving on to the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_get_word_frequency(get_word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will build the word_frequency dictionary using the training set. \n",
    "word_frequency = get_word_frequency(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restrictions': {'spam': 3, 'ham': 12},\n",
       " 'find': {'spam': 105, 'ham': 403},\n",
       " 'attached': {'spam': 10, 'ham': 574},\n",
       " 'amy': {'spam': 1, 'ham': 38},\n",
       " 'useful': {'spam': 8, 'ham': 101},\n",
       " 'would': {'spam': 166, 'ham': 1525},\n",
       " 'help': {'spam': 129, 'ham': 557},\n",
       " 'vince': {'spam': 2, 'ham': 2223},\n",
       " 'waterloo': {'spam': 1, 'ham': 11},\n",
       " 'lcd': {'spam': 1, 'ham': 7},\n",
       " 'great': {'spam': 113, 'ham': 358},\n",
       " 'bio': {'spam': 2, 'ham': 24},\n",
       " '50': {'spam': 118, 'ham': 171},\n",
       " 'audio': {'spam': 7, 'ham': 17},\n",
       " '7562': {'spam': 1, 'ham': 16},\n",
       " 'shirley': {'spam': 2, 'ham': 585},\n",
       " 'enron': {'spam': 1, 'ham': 2066},\n",
       " 'conference': {'spam': 5, 'ham': 383},\n",
       " 'information': {'spam': 270, 'ham': 723},\n",
       " 'cc': {'spam': 5, 'ham': 1719},\n",
       " 'th': {'spam': 45, 'ham': 522},\n",
       " 'hotel': {'spam': 8, 'ham': 117},\n",
       " 'exactly': {'spam': 26, 'ham': 39},\n",
       " 'short': {'spam': 38, 'ham': 181},\n",
       " 'weeks': {'spam': 38, 'ham': 221},\n",
       " 'send': {'spam': 131, 'ham': 496},\n",
       " 'reserve': {'spam': 11, 'ham': 67},\n",
       " 'papers': {'spam': 4, 'ham': 59},\n",
       " 'energy': {'spam': 22, 'ham': 571},\n",
       " 'quickly': {'spam': 30, 'ham': 68},\n",
       " 'lap': {'spam': 3, 'ham': 6},\n",
       " 'presentation': {'spam': 14, 'ham': 305},\n",
       " '853': {'spam': 2, 'ham': 303},\n",
       " 'wishes': {'spam': 4, 'ham': 53},\n",
       " 'abstract': {'spam': 2, 'ham': 23},\n",
       " 'dietary': {'spam': 2, 'ham': 3},\n",
       " 'tap': {'spam': 2, 'ham': 10},\n",
       " 'thank': {'spam': 69, 'ham': 669},\n",
       " 'studies': {'spam': 8, 'ham': 50},\n",
       " 'gl': {'spam': 5, 'ham': 7},\n",
       " '2': {'spam': 209, 'ham': 918},\n",
       " '3': {'spam': 246, 'ham': 839},\n",
       " '5728': {'spam': 1, 'ham': 5},\n",
       " '``': {'spam': 224, 'ham': 1140},\n",
       " 'compile': {'spam': 1, 'ham': 9},\n",
       " 'dinner': {'spam': 1, 'ham': 162},\n",
       " 'room': {'spam': 8, 'ham': 188},\n",
       " 'speakers': {'spam': 2, 'ham': 66},\n",
       " 'joining': {'spam': 2, 'ham': 67},\n",
       " '519': {'spam': 1, 'ham': 7},\n",
       " 'tel': {'spam': 16, 'ham': 170},\n",
       " 'hook': {'spam': 1, 'ham': 9},\n",
       " 'requests': {'spam': 14, 'ham': 58},\n",
       " 'finance': {'spam': 11, 'ham': 340},\n",
       " '28': {'spam': 14, 'ham': 298},\n",
       " 'sending': {'spam': 33, 'ham': 162},\n",
       " '00': {'spam': 86, 'ham': 616},\n",
       " 'staying': {'spam': 2, 'ham': 29},\n",
       " 'may': {'spam': 201, 'ham': 847},\n",
       " 'day': {'spam': 133, 'ham': 467},\n",
       " 'good': {'spam': 147, 'ham': 519},\n",
       " '03': {'spam': 19, 'ham': 552},\n",
       " '713': {'spam': 1, 'ham': 507},\n",
       " 'contribution': {'spam': 5, 'ham': 50},\n",
       " 'derivatives': {'spam': 1, 'ham': 210},\n",
       " 'appreciated': {'spam': 3, 'ham': 82},\n",
       " 'ca': {'spam': 31, 'ham': 120},\n",
       " 'aaldous': {'spam': 1, 'ham': 5},\n",
       " '2000': {'spam': 23, 'ham': 1377},\n",
       " 'requirements': {'spam': 21, 'ham': 91},\n",
       " 'university': {'spam': 6, 'ham': 465},\n",
       " 'aldous': {'spam': 1, 'ham': 5},\n",
       " 'dr': {'spam': 12, 'ham': 198},\n",
       " 'best': {'spam': 254, 'ham': 570},\n",
       " '31': {'spam': 24, 'ham': 255},\n",
       " 'please': {'spam': 291, 'ham': 1949},\n",
       " 'looking': {'spam': 87, 'ham': 295},\n",
       " 'organizers': {'spam': 1, 'ham': 5},\n",
       " 'fax': {'spam': 34, 'ham': 515},\n",
       " 'much': {'spam': 100, 'ham': 443},\n",
       " '30': {'spam': 90, 'ham': 590},\n",
       " 'toronto': {'spam': 2, 'ham': 11},\n",
       " 'participants': {'spam': 9, 'ham': 94},\n",
       " 'several': {'spam': 45, 'ham': 222},\n",
       " 'morning': {'spam': 30, 'ham': 356},\n",
       " 'nights': {'spam': 3, 'ham': 13},\n",
       " '29': {'spam': 45, 'ham': 285},\n",
       " 'let': {'spam': 54, 'ham': 1035},\n",
       " 'vincent': {'spam': 2, 'ham': 122},\n",
       " '0600': {'spam': 2, 'ham': 58},\n",
       " 'shortly': {'spam': 9, 'ham': 40},\n",
       " 'video': {'spam': 21, 'ham': 44},\n",
       " 'distribution': {'spam': 21, 'ham': 94},\n",
       " 'wrote': {'spam': 1, 'ham': 178},\n",
       " 'doc': {'spam': 3, 'ham': 367},\n",
       " 'centre': {'spam': 1, 'ham': 24},\n",
       " 'kaminski': {'spam': 1, 'ham': 1564},\n",
       " '5290': {'spam': 1, 'ham': 146},\n",
       " 'planning': {'spam': 15, 'ham': 115},\n",
       " 'subject': {'spam': 148, 'ham': 1805},\n",
       " 'ordinator': {'spam': 1, 'ham': 12},\n",
       " '888': {'spam': 11, 'ham': 31},\n",
       " 'need': {'spam': 182, 'ham': 774},\n",
       " 'ms': {'spam': 10, 'ham': 105},\n",
       " 'projector': {'spam': 2, 'ham': 14},\n",
       " 'also': {'spam': 129, 'ham': 894},\n",
       " 'equipment': {'spam': 23, 'ham': 44},\n",
       " 'see': {'spam': 214, 'ham': 651},\n",
       " 'file': {'spam': 25, 'ham': 215},\n",
       " '11': {'spam': 52, 'ham': 753},\n",
       " 'ext': {'spam': 8, 'ham': 79},\n",
       " 'reservations': {'spam': 3, 'ham': 44},\n",
       " 'uwaterloo': {'spam': 1, 'ham': 9},\n",
       " 'know': {'spam': 152, 'ham': 1230},\n",
       " 'crenshaw': {'spam': 1, 'ham': 429},\n",
       " 'co': {'spam': 33, 'ham': 206},\n",
       " 'soon': {'spam': 45, 'ham': 292},\n",
       " 'following': {'spam': 107, 'ham': 530},\n",
       " 'possible': {'spam': 51, 'ham': 324},\n",
       " 'com': {'spam': 381, 'ham': 1195},\n",
       " '10': {'spam': 181, 'ham': 953},\n",
       " '4567': {'spam': 1, 'ham': 5},\n",
       " 'special': {'spam': 93, 'ham': 118},\n",
       " 'n': {'spam': 44, 'ham': 66},\n",
       " 'email': {'spam': 254, 'ham': 610},\n",
       " 'l': {'spam': 93, 'ham': 139},\n",
       " 'thanks': {'spam': 47, 'ham': 1429},\n",
       " 'advanced': {'spam': 21, 'ham': 45},\n",
       " 'refer': {'spam': 5, 'ham': 28},\n",
       " 'event': {'spam': 11, 'ham': 150},\n",
       " 'maths': {'spam': 1, 'ham': 14},\n",
       " 'part': {'spam': 70, 'ham': 259},\n",
       " 'course': {'spam': 27, 'ham': 201},\n",
       " 'copy': {'spam': 55, 'ham': 290},\n",
       " 'draft': {'spam': 2, 'ham': 102},\n",
       " 'case': {'spam': 29, 'ham': 240},\n",
       " 'financial': {'spam': 72, 'ham': 289},\n",
       " 'finmathmail': {'spam': 1, 'ham': 2},\n",
       " 'paul': {'spam': 3, 'ham': 111},\n",
       " 'monday': {'spam': 8, 'ham': 347},\n",
       " 'j': {'spam': 55, 'ham': 1282},\n",
       " 'mailto': {'spam': 8, 'ham': 136},\n",
       " 'aol': {'spam': 11, 'ham': 94},\n",
       " 'power': {'spam': 41, 'ham': 405},\n",
       " 'interesting': {'spam': 11, 'ham': 122},\n",
       " 'modifications': {'spam': 4, 'ham': 22},\n",
       " 'points': {'spam': 7, 'ham': 92},\n",
       " 'pbristow': {'spam': 1, 'ham': 5},\n",
       " 'messages': {'spam': 14, 'ham': 57},\n",
       " 'way': {'spam': 178, 'ham': 299},\n",
       " 'every': {'spam': 75, 'ham': 109},\n",
       " 'week': {'spam': 80, 'ham': 621},\n",
       " 'riskwaters': {'spam': 1, 'ham': 16},\n",
       " '09': {'spam': 11, 'ham': 526},\n",
       " 'delay': {'spam': 6, 'ham': 79},\n",
       " 'certainly': {'spam': 5, 'ham': 63},\n",
       " 'hi': {'spam': 45, 'ham': 511},\n",
       " 'message': {'spam': 213, 'ham': 673},\n",
       " 'gets': {'spam': 16, 'ham': 38},\n",
       " 'bullet': {'spam': 2, 'ham': 13},\n",
       " 'respond': {'spam': 18, 'ham': 191},\n",
       " 'responding': {'spam': 3, 'ham': 37},\n",
       " 'original': {'spam': 58, 'ham': 347},\n",
       " '9': {'spam': 74, 'ham': 334},\n",
       " 'modified': {'spam': 5, 'ham': 17},\n",
       " 'apologies': {'spam': 11, 'ham': 25},\n",
       " '2001': {'spam': 21, 'ham': 864},\n",
       " 'promise': {'spam': 34, 'ham': 13},\n",
       " 'april': {'spam': 5, 'ham': 292},\n",
       " 'bullets': {'spam': 1, 'ham': 4},\n",
       " 'vkaminski': {'spam': 1, 'ham': 68},\n",
       " 'red': {'spam': 6, 'ham': 26},\n",
       " 'regarding': {'spam': 36, 'ham': 416},\n",
       " 'sent': {'spam': 110, 'ham': 573},\n",
       " 'consistently': {'spam': 7, 'ham': 12},\n",
       " 'publication': {'spam': 15, 'ham': 44},\n",
       " 'idea': {'spam': 21, 'ham': 125},\n",
       " '1976': {'spam': 2, 'ham': 9},\n",
       " 'london': {'spam': 10, 'ham': 320},\n",
       " '723': {'spam': 2, 'ham': 25},\n",
       " 'preparing': {'spam': 1, 'ham': 33},\n",
       " 'phone': {'spam': 84, 'ham': 459},\n",
       " 'models': {'spam': 6, 'ham': 154},\n",
       " 'edu': {'spam': 3, 'ham': 460},\n",
       " 'duffie': {'spam': 1, 'ham': 27},\n",
       " 'slow': {'spam': 3, 'ham': 21},\n",
       " 'swaps': {'spam': 1, 'ham': 44},\n",
       " 'read': {'spam': 58, 'ham': 126},\n",
       " 'going': {'spam': 53, 'ham': 302},\n",
       " '08': {'spam': 15, 'ham': 520},\n",
       " '80': {'spam': 42, 'ham': 30},\n",
       " 'explanations': {'spam': 1, 'ham': 4},\n",
       " '07': {'spam': 24, 'ham': 393},\n",
       " 'guidance': {'spam': 6, 'ham': 38},\n",
       " 'www': {'spam': 196, 'ham': 385},\n",
       " 'bank': {'spam': 63, 'ham': 64},\n",
       " 'darrell': {'spam': 1, 'ham': 14},\n",
       " 'pricing': {'spam': 11, 'ham': 225},\n",
       " '5015': {'spam': 1, 'ham': 8},\n",
       " 'think': {'spam': 41, 'ham': 521},\n",
       " 'able': {'spam': 46, 'ham': 327},\n",
       " '650': {'spam': 2, 'ham': 71},\n",
       " 'articles': {'spam': 6, 'ham': 43},\n",
       " 'time': {'spam': 257, 'ham': 1059},\n",
       " 'model': {'spam': 5, 'ham': 365},\n",
       " 'trading': {'spam': 28, 'ham': 350},\n",
       " 'according': {'spam': 27, 'ham': 54},\n",
       " 'stanford': {'spam': 3, 'ham': 101},\n",
       " 'spreads': {'spam': 3, 'ham': 26},\n",
       " 'documents': {'spam': 14, 'ham': 68},\n",
       " '94305': {'spam': 1, 'ham': 38},\n",
       " 'money': {'spam': 232, 'ham': 74},\n",
       " 'http': {'spam': 325, 'ham': 453},\n",
       " 'default': {'spam': 1, 'ham': 52},\n",
       " 'web': {'spam': 127, 'ham': 230},\n",
       " 'wider': {'spam': 1, 'ham': 8},\n",
       " 'still': {'spam': 45, 'ham': 286},\n",
       " 'hope': {'spam': 19, 'ham': 457},\n",
       " 'weekend': {'spam': 3, 'ham': 116},\n",
       " '38': {'spam': 8, 'ham': 96},\n",
       " 'check': {'spam': 77, 'ham': 185},\n",
       " 'owe': {'spam': 7, 'ham': 16},\n",
       " 'reviews': {'spam': 10, 'ham': 21},\n",
       " '725': {'spam': 2, 'ham': 41},\n",
       " 'notes': {'spam': 6, 'ham': 92},\n",
       " 'basis': {'spam': 24, 'ham': 125},\n",
       " 'number': {'spam': 100, 'ham': 432},\n",
       " 'asset': {'spam': 7, 'ham': 73},\n",
       " 'technical': {'spam': 12, 'ham': 184},\n",
       " 'mail': {'spam': 230, 'ham': 605},\n",
       " 'shall': {'spam': 25, 'ham': 452},\n",
       " 'review': {'spam': 34, 'ham': 331},\n",
       " 'america': {'spam': 25, 'ham': 160},\n",
       " 'satisfactory': {'spam': 2, 'ham': 18},\n",
       " 'gsb': {'spam': 1, 'ham': 13},\n",
       " 'usa': {'spam': 34, 'ham': 42},\n",
       " 'option': {'spam': 7, 'ham': 216},\n",
       " 'invoices': {'spam': 1, 'ham': 9},\n",
       " '7979': {'spam': 1, 'ham': 10},\n",
       " 'swap': {'spam': 1, 'ham': 35},\n",
       " 'related': {'spam': 25, 'ham': 196},\n",
       " 'credit': {'spam': 73, 'ham': 221},\n",
       " 'curious': {'spam': 1, 'ham': 9},\n",
       " 'var': {'spam': 4, 'ham': 147},\n",
       " 'figures': {'spam': 3, 'ham': 35},\n",
       " 'elena': {'spam': 1, 'ham': 42},\n",
       " 'obtained': {'spam': 13, 'ham': 24},\n",
       " 'gas': {'spam': 17, 'ham': 253},\n",
       " 'steven': {'spam': 1, 'ham': 114},\n",
       " 'gone': {'spam': 15, 'ham': 35},\n",
       " 'arbitrage': {'spam': 1, 'ham': 13},\n",
       " 'give': {'spam': 85, 'ham': 449},\n",
       " 'consequent': {'spam': 1, 'ham': 2},\n",
       " 'alex': {'spam': 3, 'ham': 113},\n",
       " 'others': {'spam': 72, 'ham': 100},\n",
       " 'presentations': {'spam': 5, 'ham': 103},\n",
       " '34': {'spam': 21, 'ham': 87},\n",
       " 'lon': {'spam': 1, 'ham': 181},\n",
       " 'question': {'spam': 18, 'ham': 129},\n",
       " 'giving': {'spam': 25, 'ham': 85},\n",
       " 'illustrative': {'spam': 1, 'ham': 3},\n",
       " 'markets': {'spam': 13, 'ham': 291},\n",
       " 'correlation': {'spam': 2, 'ham': 42},\n",
       " 'ect': {'spam': 2, 'ham': 1270},\n",
       " 'colleague': {'spam': 2, 'ham': 62},\n",
       " 'historical': {'spam': 15, 'ham': 89},\n",
       " 'forward': {'spam': 75, 'ham': 731},\n",
       " 'among': {'spam': 41, 'ham': 88},\n",
       " 'contact': {'spam': 107, 'ham': 623},\n",
       " 'data': {'spam': 49, 'ham': 303},\n",
       " 'didier': {'spam': 1, 'ham': 2},\n",
       " 'queries': {'spam': 1, 'ham': 23},\n",
       " 'increase': {'spam': 81, 'ham': 68},\n",
       " 'leppard': {'spam': 1, 'ham': 84},\n",
       " 'area': {'spam': 38, 'ham': 186},\n",
       " 'fwd': {'spam': 3, 'ham': 53},\n",
       " 'hou': {'spam': 1, 'ham': 1194},\n",
       " 'talk': {'spam': 11, 'ham': 345},\n",
       " '13': {'spam': 27, 'ham': 281},\n",
       " 'convergence': {'spam': 1, 'ham': 9},\n",
       " 'internal': {'spam': 5, 'ham': 81},\n",
       " 'many': {'spam': 120, 'ham': 375},\n",
       " 'paulo': {'spam': 1, 'ham': 84},\n",
       " 'magne': {'spam': 1, 'ham': 2},\n",
       " 'steve': {'spam': 5, 'ham': 194},\n",
       " 'chilkina': {'spam': 1, 'ham': 37},\n",
       " 'grant': {'spam': 8, 'ham': 198},\n",
       " 'sensitive': {'spam': 7, 'ham': 23},\n",
       " 'curves': {'spam': 1, 'ham': 105},\n",
       " 'represent': {'spam': 21, 'ham': 34},\n",
       " 'concetta': {'spam': 2, 'ham': 1},\n",
       " 'regards': {'spam': 106, 'ham': 754},\n",
       " 'walker': {'spam': 6, 'ham': 1},\n",
       " 'found': {'spam': 47, 'ham': 113},\n",
       " '4': {'spam': 147, 'ham': 635},\n",
       " 'around': {'spam': 40, 'ham': 177},\n",
       " '55': {'spam': 18, 'ham': 126},\n",
       " 'id': {'spam': 67, 'ham': 113},\n",
       " 'free': {'spam': 197, 'ham': 345},\n",
       " '06': {'spam': 17, 'ham': 414},\n",
       " 'thought': {'spam': 26, 'ham': 110},\n",
       " 'classified': {'spam': 10, 'ham': 1},\n",
       " 'gh': {'spam': 2, 'ham': 1},\n",
       " 'wd': {'spam': 2, 'ham': 5},\n",
       " 'wraps': {'spam': 4, 'ham': 2},\n",
       " 'batistich': {'spam': 2, 'ham': 1},\n",
       " 'otswkuk': {'spam': 2, 'ham': 1},\n",
       " 'internet': {'spam': 100, 'ham': 167},\n",
       " '20': {'spam': 86, 'ham': 437},\n",
       " '2005': {'spam': 129, 'ham': 4},\n",
       " 'hello': {'spam': 108, 'ham': 210},\n",
       " 'safe': {'spam': 36, 'ham': 19},\n",
       " 'site': {'spam': 175, 'ham': 175},\n",
       " 'ads': {'spam': 13, 'ham': 1},\n",
       " 'graand': {'spam': 7, 'ham': 1},\n",
       " '05': {'spam': 19, 'ham': 518},\n",
       " 'called': {'spam': 24, 'ham': 105},\n",
       " '687': {'spam': 2, 'ham': 1},\n",
       " 'place': {'spam': 43, 'ham': 216},\n",
       " 'invite': {'spam': 13, 'ham': 143},\n",
       " '18': {'spam': 45, 'ham': 307},\n",
       " 'effective': {'spam': 53, 'ham': 79},\n",
       " '1': {'spam': 288, 'ham': 890},\n",
       " 'august': {'spam': 8, 'ham': 121},\n",
       " 'support': {'spam': 63, 'ham': 298},\n",
       " 'research': {'spam': 34, 'ham': 913},\n",
       " '7094': {'spam': 1, 'ham': 6},\n",
       " '5': {'spam': 268, 'ham': 568},\n",
       " 'update': {'spam': 22, 'ham': 179},\n",
       " 'becky': {'spam': 1, 'ham': 13},\n",
       " '17': {'spam': 29, 'ham': 344},\n",
       " 'pm': {'spam': 12, 'ham': 1333},\n",
       " 'teams': {'spam': 4, 'ham': 42},\n",
       " '26': {'spam': 14, 'ham': 268},\n",
       " 'official': {'spam': 13, 'ham': 75},\n",
       " 'thanx': {'spam': 2, 'ham': 12},\n",
       " 'sheet': {'spam': 2, 'ham': 33},\n",
       " 'ena': {'spam': 1, 'ham': 84},\n",
       " 'allocation': {'spam': 4, 'ham': 27},\n",
       " 'allocations': {'spam': 1, 'ham': 14},\n",
       " 'call': {'spam': 71, 'ham': 667},\n",
       " 'pham': {'spam': 1, 'ham': 11},\n",
       " 'tell': {'spam': 33, 'ham': 116},\n",
       " '01': {'spam': 25, 'ham': 708},\n",
       " 'take': {'spam': 122, 'ham': 457},\n",
       " 'look': {'spam': 114, 'ham': 620},\n",
       " '04': {'spam': 8, 'ham': 592},\n",
       " '12': {'spam': 92, 'ham': 651},\n",
       " 'questions': {'spam': 39, 'ham': 561},\n",
       " 'commercial': {'spam': 24, 'ham': 94},\n",
       " 'forwarded': {'spam': 4, 'ham': 908},\n",
       " '45': {'spam': 43, 'ham': 183},\n",
       " 'telephone': {'spam': 23, 'ham': 141},\n",
       " 'piccadilly': {'spam': 1, 'ham': 5},\n",
       " '6': {'spam': 116, 'ham': 386},\n",
       " 'left': {'spam': 22, 'ham': 114},\n",
       " 'x': {'spam': 77, 'ham': 277},\n",
       " '35383': {'spam': 1, 'ham': 39},\n",
       " '7491': {'spam': 1, 'ham': 3},\n",
       " 'like': {'spam': 223, 'ham': 992},\n",
       " 'table': {'spam': 8, 'ham': 62},\n",
       " 'ahmad': {'spam': 1, 'ham': 51},\n",
       " '6321': {'spam': 1, 'ham': 3},\n",
       " '499': {'spam': 6, 'ham': 7},\n",
       " 'parsons': {'spam': 1, 'ham': 41},\n",
       " 'p': {'spam': 83, 'ham': 368},\n",
       " 'park': {'spam': 10, 'ham': 23},\n",
       " 'developments': {'spam': 7, 'ham': 54},\n",
       " 'restaurant': {'spam': 2, 'ham': 32},\n",
       " 'opportunity': {'spam': 63, 'ham': 296},\n",
       " 'anjam': {'spam': 1, 'ham': 103},\n",
       " 'slots': {'spam': 1, 'ham': 11},\n",
       " 'ben': {'spam': 3, 'ham': 66},\n",
       " 'hyde': {'spam': 1, 'ham': 8},\n",
       " '85': {'spam': 20, 'ham': 18},\n",
       " 'changed': {'spam': 12, 'ham': 70},\n",
       " 'benjamin': {'spam': 1, 'ham': 15},\n",
       " '0171': {'spam': 1, 'ham': 6},\n",
       " 'hd': {'spam': 1, 'ham': 3},\n",
       " '48': {'spam': 18, 'ham': 91},\n",
       " 'lane': {'spam': 2, 'ham': 15},\n",
       " 'sure': {'spam': 64, 'ham': 271},\n",
       " 'seems': {'spam': 6, 'ham': 131},\n",
       " 'make': {'spam': 227, 'ham': 563},\n",
       " '2222': {'spam': 1, 'ham': 3},\n",
       " 'schedule': {'spam': 10, 'ham': 357},\n",
       " 'underground': {'spam': 1, 'ham': 4},\n",
       " 'yards': {'spam': 1, 'ham': 4},\n",
       " 'sunday': {'spam': 1, 'ham': 63},\n",
       " 'latest': {'spam': 36, 'ham': 103},\n",
       " '15': {'spam': 62, 'ham': 478},\n",
       " 'wlv': {'spam': 2, 'ham': 5},\n",
       " 'evening': {'spam': 3, 'ham': 109},\n",
       " 'booked': {'spam': 1, 'ham': 12},\n",
       " '14': {'spam': 38, 'ham': 306},\n",
       " '020': {'spam': 1, 'ham': 13},\n",
       " 'invitation': {'spam': 8, 'ham': 111},\n",
       " '02': {'spam': 14, 'ham': 564},\n",
       " 'diverso': {'spam': 1, 'ham': 3},\n",
       " 'name': {'spam': 117, 'ham': 318},\n",
       " 'close': {'spam': 16, 'ham': 97},\n",
       " 'early': {'spam': 13, 'ham': 152},\n",
       " 'discuss': {'spam': 13, 'ham': 332},\n",
       " 'one': {'spam': 295, 'ham': 773},\n",
       " 'accordingly': {'spam': 4, 'ham': 22},\n",
       " 'corner': {'spam': 9, 'ham': 18},\n",
       " 'pinnamaneni': {'spam': 1, 'ham': 81},\n",
       " 'describe': {'spam': 2, 'ham': 21},\n",
       " 'first': {'spam': 88, 'ham': 391},\n",
       " 'operation': {'spam': 13, 'ham': 46},\n",
       " 'could': {'spam': 114, 'ham': 633},\n",
       " 'continuing': {'spam': 9, 'ham': 47},\n",
       " 'sit': {'spam': 5, 'ham': 40},\n",
       " 'talked': {'spam': 1, 'ham': 107},\n",
       " 'employee': {'spam': 8, 'ham': 124},\n",
       " 'mentality': {'spam': 1, 'ham': 3},\n",
       " 'working': {'spam': 52, 'ham': 447},\n",
       " 'role': {'spam': 4, 'ham': 131},\n",
       " 'db': {'spam': 6, 'ham': 9},\n",
       " 'lewis': {'spam': 2, 'ham': 25},\n",
       " 'feel': {'spam': 60, 'ham': 293},\n",
       " 'works': {'spam': 31, 'ham': 121},\n",
       " '44': {'spam': 11, 'ham': 216},\n",
       " 'ees': {'spam': 1, 'ham': 122},\n",
       " 'little': {'spam': 36, 'ham': 99},\n",
       " 'alternative': {'spam': 18, 'ham': 62},\n",
       " 'leadership': {'spam': 3, 'ham': 44},\n",
       " 'withing': {'spam': 1, 'ham': 3},\n",
       " 'programing': {'spam': 1, 'ham': 4},\n",
       " 'person': {'spam': 37, 'ham': 251},\n",
       " 'econometrics': {'spam': 1, 'ham': 10},\n",
       " 'ancillary': {'spam': 3, 'ham': 9},\n",
       " 'start': {'spam': 132, 'ham': 193},\n",
       " 'anoush': {'spam': 1, 'ham': 5},\n",
       " 'engineer': {'spam': 1, 'ham': 20},\n",
       " 'result': {'spam': 58, 'ham': 87},\n",
       " '22': {'spam': 25, 'ham': 274},\n",
       " 'bringing': {'spam': 7, 'ham': 31},\n",
       " 'boss': {'spam': 9, 'ham': 18},\n",
       " 'three': {'spam': 69, 'ham': 161},\n",
       " 'benevides': {'spam': 1, 'ham': 10},\n",
       " 'focus': {'spam': 27, 'ham': 95},\n",
       " 'suspect': {'spam': 4, 'ham': 19},\n",
       " 'suggestion': {'spam': 1, 'ham': 40},\n",
       " 'plants': {'spam': 5, 'ham': 37},\n",
       " 'bridge': {'spam': 2, 'ham': 19},\n",
       " 'krishna': {'spam': 1, 'ham': 88},\n",
       " 'forever': {'spam': 12, 'ham': 7},\n",
       " 'fix': {'spam': 4, 'ham': 16},\n",
       " 'hire': {'spam': 2, 'ham': 49},\n",
       " 'applications': {'spam': 22, 'ham': 96},\n",
       " 'applying': {'spam': 4, 'ham': 38},\n",
       " 'meant': {'spam': 10, 'ham': 11},\n",
       " 'spreadsheet': {'spam': 4, 'ham': 97},\n",
       " 'management': {'spam': 67, 'ham': 553},\n",
       " 'assessment': {'spam': 3, 'ham': 33},\n",
       " 'operations': {'spam': 13, 'ham': 133},\n",
       " 'side': {'spam': 13, 'ham': 104},\n",
       " 'krishnarao': {'spam': 1, 'ham': 103},\n",
       " 'directly': {'spam': 20, 'ham': 132},\n",
       " 'surprising': {'spam': 2, 'ham': 6},\n",
       " '35485': {'spam': 1, 'ham': 4},\n",
       " 'expressed': {'spam': 12, 'ham': 53},\n",
       " 'james': {'spam': 4, 'ham': 91},\n",
       " 'neil': {'spam': 1, 'ham': 30},\n",
       " 'economics': {'spam': 3, 'ham': 78},\n",
       " 'ronnie': {'spam': 1, 'ham': 18},\n",
       " 'departure': {'spam': 1, 'ham': 26},\n",
       " 'excited': {'spam': 3, 'ham': 59},\n",
       " 'overall': {'spam': 5, 'ham': 75},\n",
       " 'asked': {'spam': 15, 'ham': 208},\n",
       " 'people': {'spam': 127, 'ham': 324},\n",
       " 'particularly': {'spam': 8, 'ham': 50},\n",
       " 'regulatory': {'spam': 5, 'ham': 54},\n",
       " 'excel': {'spam': 2, 'ham': 55},\n",
       " 'creative': {'spam': 91, 'ham': 12},\n",
       " 'facilitate': {'spam': 1, 'ham': 53},\n",
       " 'leader': {'spam': 20, 'ham': 33},\n",
       " 'anything': {'spam': 31, 'ham': 145},\n",
       " 'lack': {'spam': 11, 'ham': 36},\n",
       " 'must': {'spam': 97, 'ham': 132},\n",
       " '43': {'spam': 14, 'ham': 86},\n",
       " 'fit': {'spam': 24, 'ham': 93},\n",
       " 'phd': {'spam': 3, 'ham': 73},\n",
       " 'services': {'spam': 77, 'ham': 267},\n",
       " 'said': {'spam': 43, 'ham': 130},\n",
       " 'building': {'spam': 23, 'ham': 110},\n",
       " 'experienced': {'spam': 15, 'ham': 19},\n",
       " 'scott': {'spam': 3, 'ham': 87},\n",
       " 'described': {'spam': 10, 'ham': 34},\n",
       " 'hk': {'spam': 4, 'ham': 4},\n",
       " 'pacificcorp': {'spam': 1, 'ham': 3},\n",
       " 'chahal': {'spam': 1, 'ham': 12},\n",
       " 'ugh': {'spam': 1, 'ham': 7},\n",
       " 'since': {'spam': 47, 'ham': 286},\n",
       " '9609': {'spam': 1, 'ham': 3},\n",
       " 'pacificor': {'spam': 1, 'ham': 3},\n",
       " 'reliable': {'spam': 25, 'ham': 17},\n",
       " 'plant': {'spam': 5, 'ham': 68},\n",
       " 'concerned': {'spam': 4, 'ham': 55},\n",
       " 'longer': {'spam': 45, 'ham': 105},\n",
       " 'understand': {'spam': 19, 'ham': 172},\n",
       " 'systems': {'spam': 34, 'ham': 142},\n",
       " 'expertise': {'spam': 12, 'ham': 52},\n",
       " 'tie': {'spam': 2, 'ham': 14},\n",
       " 'started': {'spam': 28, 'ham': 64},\n",
       " 'together': {'spam': 24, 'ham': 247},\n",
       " 'zimin': {'spam': 1, 'ham': 201},\n",
       " 'hired': {'spam': 4, 'ham': 24},\n",
       " 'replacement': {'spam': 4, 'ham': 20},\n",
       " 'feedback': {'spam': 7, 'ham': 126},\n",
       " '35': {'spam': 29, 'ham': 124},\n",
       " 'capable': {'spam': 6, 'ham': 10},\n",
       " 'within': {'spam': 145, 'ham': 252},\n",
       " 'skill': {'spam': 1, 'ham': 20},\n",
       " 'proceed': {'spam': 3, 'ham': 76},\n",
       " 'got': {'spam': 39, 'ham': 159},\n",
       " 'well': {'spam': 80, 'ham': 541},\n",
       " 'interest': {'spam': 89, 'ham': 303},\n",
       " 'txu': {'spam': 1, 'ham': 10},\n",
       " 'offered': {'spam': 16, 'ham': 53},\n",
       " 'evaluate': {'spam': 4, 'ham': 47},\n",
       " 'tariffs': {'spam': 1, 'ham': 11},\n",
       " 'scared': {'spam': 1, 'ham': 4},\n",
       " 'analysis': {'spam': 16, 'ham': 244},\n",
       " 'go': {'spam': 136, 'ham': 352},\n",
       " 'utility': {'spam': 6, 'ham': 59},\n",
       " 'info': {'spam': 89, 'ham': 159},\n",
       " 'lu': {'spam': 12, 'ham': 146},\n",
       " 'sentence': {'spam': 1, 'ham': 12},\n",
       " 'term': {'spam': 39, 'ham': 188},\n",
       " 'explain': {'spam': 4, 'ham': 44},\n",
       " 'cons': {'spam': 1, 'ham': 8},\n",
       " 'probed': {'spam': 1, 'ham': 3},\n",
       " 'bring': {'spam': 38, 'ham': 180},\n",
       " 'rm': {'spam': 6, 'ham': 11},\n",
       " 'guy': {'spam': 3, 'ham': 59},\n",
       " 'oil': {'spam': 23, 'ham': 89},\n",
       " 'w': {'spam': 31, 'ham': 131},\n",
       " 'suggest': {'spam': 7, 'ham': 78},\n",
       " 'skills': {'spam': 5, 'ham': 112},\n",
       " 'potential': {'spam': 43, 'ham': 117},\n",
       " 'provide': {'spam': 119, 'ham': 333},\n",
       " 'risk': {'spam': 62, 'ham': 555},\n",
       " 'communicator': {'spam': 3, 'ham': 5},\n",
       " 'comes': {'spam': 19, 'ham': 52},\n",
       " 'math': {'spam': 2, 'ham': 23},\n",
       " 'hiring': {'spam': 6, 'ham': 41},\n",
       " 'group': {'spam': 53, 'ham': 935},\n",
       " 'krisna': {'spam': 1, 'ham': 3},\n",
       " 'expected': {'spam': 25, 'ham': 94},\n",
       " 'received': {'spam': 111, 'ham': 294},\n",
       " 'engineering': {'spam': 12, 'ham': 154},\n",
       " '19': {'spam': 61, 'ham': 337},\n",
       " 'development': {'spam': 33, 'ham': 298},\n",
       " 'limited': {'spam': 66, 'ham': 84},\n",
       " 'strongly': {'spam': 18, 'ham': 30},\n",
       " 'jump': {'spam': 7, 'ham': 43},\n",
       " 'mentioned': {'spam': 16, 'ham': 176},\n",
       " 'assign': {'spam': 1, 'ham': 9},\n",
       " 'ming': {'spam': 1, 'ham': 6},\n",
       " 'background': {'spam': 4, 'ham': 102},\n",
       " 'interested': {'spam': 151, 'ham': 364},\n",
       " '21': {'spam': 53, 'ham': 283},\n",
       " 'use': {'spam': 179, 'ham': 422},\n",
       " 'infrastructure': {'spam': 9, 'ham': 25},\n",
       " 'nd': {'spam': 14, 'ham': 111},\n",
       " 'communication': {'spam': 40, 'ham': 72},\n",
       " 'instead': {'spam': 43, 'ham': 46},\n",
       " 'b': {'spam': 52, 'ham': 219},\n",
       " 'took': {'spam': 17, 'ham': 64},\n",
       " 'jay': {'spam': 1, 'ham': 23},\n",
       " 'solid': {'spam': 4, 'ham': 16},\n",
       " 'dennis': {'spam': 1, 'ham': 43},\n",
       " 'pros': {'spam': 1, 'ham': 13},\n",
       " 'analyst': {'spam': 6, 'ham': 127},\n",
       " 'anyway': {'spam': 1, 'ham': 63},\n",
       " 'stoness': {'spam': 1, 'ham': 7},\n",
       " 'informations': {'spam': 6, 'ham': 1},\n",
       " 'sincerely': {'spam': 58, 'ham': 254},\n",
       " 'noticed': {'spam': 18, 'ham': 19},\n",
       " 'card': {'spam': 37, 'ham': 46},\n",
       " 'illegal': {'spam': 5, 'ham': 1},\n",
       " 'inform': {'spam': 33, 'ham': 72},\n",
       " 'various': {'spam': 10, 'ham': 84},\n",
       " 'indicates': {'spam': 2, 'ham': 17},\n",
       " 'state': {'spam': 85, 'ham': 120},\n",
       " 'customer': {'spam': 60, 'ham': 97},\n",
       " 'control': {'spam': 48, 'ham': 79},\n",
       " 'identity': {'spam': 71, 'ham': 3},\n",
       " 'verify': {'spam': 19, 'ham': 23},\n",
       " 'terrorism': {'spam': 4, 'ham': 1},\n",
       " 'business': {'spam': 273, 'ham': 524},\n",
       " 'conditions': {'spam': 18, 'ham': 39},\n",
       " 'security': {'spam': 96, 'ham': 77},\n",
       " 'past': {'spam': 48, 'ham': 112},\n",
       " 'dear': {'spam': 85, 'ham': 501},\n",
       " 'lasalle': {'spam': 6, 'ham': 1},\n",
       " 'account': {'spam': 93, 'ham': 100},\n",
       " 'terms': {'spam': 41, 'ham': 102},\n",
       " 'link': {'spam': 89, 'ham': 95},\n",
       " 'unusual': {'spam': 9, 'ham': 8},\n",
       " 'consideration': {'spam': 7, 'ham': 67},\n",
       " 'follow': {'spam': 30, 'ham': 181},\n",
       " 'visa': {'spam': 19, 'ham': 28},\n",
       " 'drugs': {'spam': 16, 'ham': 1},\n",
       " 'involved': {'spam': 26, 'ham': 132},\n",
       " 'outside': {'spam': 14, 'ham': 84},\n",
       " 'activity': {'spam': 14, 'ham': 27},\n",
       " 'complete': {'spam': 62, 'ham': 142},\n",
       " 'title': {'spam': 16, 'ham': 88},\n",
       " 'instructed': {'spam': 6, 'ham': 4},\n",
       " 'access': {'spam': 79, 'ham': 267},\n",
       " 'note': {'spam': 53, 'ham': 248},\n",
       " 'violations': {'spam': 2, 'ham': 3},\n",
       " 'agreed': {'spam': 15, 'ham': 78},\n",
       " 'liability': {'spam': 10, 'ham': 22},\n",
       " 'laundering': {'spam': 2, 'ham': 1},\n",
       " 'verification': {'spam': 17, 'ham': 9},\n",
       " 'always': {'spam': 44, 'ham': 100},\n",
       " 'notified': {'spam': 7, 'ham': 26},\n",
       " 'parties': {'spam': 23, 'ham': 36},\n",
       " 'times': {'spam': 39, 'ham': 149},\n",
       " 'externally': {'spam': 2, 'ham': 4},\n",
       " 'compromised': {'spam': 6, 'ham': 2},\n",
       " 'occurred': {'spam': 3, 'ham': 9},\n",
       " 'transactions': {'spam': 26, 'ham': 60},\n",
       " 'matter': {'spam': 40, 'ham': 72},\n",
       " 'allowed': {'spam': 13, 'ham': 28},\n",
       " 'designate': {'spam': 2, 'ham': 6},\n",
       " 'accounts': {'spam': 21, 'ham': 37},\n",
       " 'aware': {'spam': 22, 'ham': 56},\n",
       " 'expedited': {'spam': 2, 'ham': 2},\n",
       " 'unlock': {'spam': 2, 'ham': 1},\n",
       " 'department': {'spam': 22, 'ham': 299},\n",
       " 'process': {'spam': 44, 'ham': 322},\n",
       " 'federal': {'spam': 20, 'ham': 40},\n",
       " 'reactivate': {'spam': 2, 'ham': 1},\n",
       " 'failure': {'spam': 30, 'ham': 19},\n",
       " 'block': {'spam': 7, 'ham': 19},\n",
       " 'initiated': {'spam': 6, 'ham': 8},\n",
       " 'lf': {'spam': 21, 'ham': 1},\n",
       " 'sales': {'spam': 81, 'ham': 60},\n",
       " 'oniy': {'spam': 27, 'ham': 1},\n",
       " 'invested': {'spam': 33, 'ham': 4},\n",
       " 'invisible': {'spam': 20, 'ham': 1},\n",
       " 'jacindamaddox': {'spam': 2, 'ham': 1},\n",
       " 'peopie': {'spam': 23, 'ham': 1},\n",
       " 'dramatically': {'spam': 32, 'ham': 19},\n",
       " 'visitors': {'spam': 36, 'ham': 4},\n",
       " 'search': {'spam': 75, 'ham': 45},\n",
       " 'depend': {'spam': 10, 'ham': 10},\n",
       " 'submit': {'spam': 38, 'ham': 59},\n",
       " 'efforts': {'spam': 62, 'ham': 108},\n",
       " 'success': {'spam': 91, 'ham': 108},\n",
       " 'oniine': {'spam': 51, 'ham': 1},\n",
       " 'engines': {'spam': 52, 'ham': 4},\n",
       " 'simply': {'spam': 81, 'ham': 50},\n",
       " 'e': {'spam': 245, 'ham': 684},\n",
       " 'spent': {'spam': 35, 'ham': 47},\n",
       " 'virtualiy': {'spam': 10, 'ham': 1},\n",
       " 'revenues': {'spam': 49, 'ham': 11},\n",
       " 'visibie': {'spam': 12, 'ham': 1},\n",
       " 'vain': {'spam': 28, 'ham': 1},\n",
       " 'boost': {'spam': 60, 'ham': 5},\n",
       " 'places': {'spam': 23, 'ham': 8},\n",
       " 'online': {'spam': 156, 'ham': 92},\n",
       " 'watch': {'spam': 55, 'ham': 19},\n",
       " 'otherwise': {'spam': 32, 'ham': 77},\n",
       " 'stream': {'spam': 31, 'ham': 15},\n",
       " 'want': {'spam': 214, 'ham': 416},\n",
       " 'means': {'spam': 45, 'ham': 72},\n",
       " 'website': {'spam': 170, 'ham': 111},\n",
       " 'submitting': {'spam': 33, 'ham': 11},\n",
       " 'multiple': {'spam': 28, 'ham': 47},\n",
       " 'fx': {'spam': 3, 'ham': 18},\n",
       " 'lee': {'spam': 3, 'ham': 72},\n",
       " 'value': {'spam': 46, 'ham': 209},\n",
       " 'levels': {'spam': 6, 'ham': 39},\n",
       " 'pessimistic': {'spam': 1, 'ham': 4},\n",
       " 'debt': {'spam': 27, 'ham': 44},\n",
       " 'stock': {'spam': 48, 'ham': 52},\n",
       " 'cases': {'spam': 8, 'ham': 37},\n",
       " 'probability': {'spam': 1, 'ham': 15},\n",
       " 'joint': {'spam': 13, 'ham': 35},\n",
       " 'given': {'spam': 50, 'ham': 256},\n",
       " 'rab': {'spam': 1, 'ham': 7},\n",
       " 'hence': {'spam': 11, 'ham': 34},\n",
       " 'michael': {'spam': 11, 'ham': 154},\n",
       " 'higher': {'spam': 23, 'ham': 77},\n",
       " 'bob': {'spam': 3, 'ham': 109},\n",
       " 'price': {'spam': 107, 'ham': 339},\n",
       " 'lower': {'spam': 31, 'ham': 47},\n",
       " 'reaching': {'spam': 1, 'ham': 18},\n",
       " 'updated': {'spam': 17, 'ham': 88},\n",
       " 'probabilities': {'spam': 1, 'ham': 22},\n",
       " '35163': {'spam': 1, 'ham': 5},\n",
       " 'optimistic': {'spam': 1, 'ham': 7},\n",
       " 'york': {'spam': 13, 'ham': 88},\n",
       " 'motivations': {'spam': 2, 'ham': 3},\n",
       " 'nymex': {'spam': 2, 'ham': 28},\n",
       " 'registration': {'spam': 16, 'ham': 60},\n",
       " 'real': {'spam': 93, 'ham': 169},\n",
       " 'options': {'spam': 24, 'ham': 255},\n",
       " 'current': {'spam': 73, 'ham': 254},\n",
       " 'contractual': {'spam': 3, 'ham': 14},\n",
       " 'unsubscribe': {'spam': 68, 'ham': 33},\n",
       " 'energyinstitution': {'spam': 2, 'ham': 1},\n",
       " 'terminology': {'spam': 2, 'ham': 1},\n",
       " 'structuring': {'spam': 2, 'ham': 40},\n",
       " 'deal': {'spam': 39, 'ham': 159},\n",
       " 'buy': {'spam': 97, 'ham': 110},\n",
       " 'two': {'spam': 66, 'ham': 465},\n",
       " 'practical': {'spam': 7, 'ham': 44},\n",
       " '1207': {'spam': 2, 'ham': 2},\n",
       " 'class': {'spam': 10, 'ham': 102},\n",
       " 'operational': {'spam': 4, 'ham': 50},\n",
       " 'bird': {'spam': 6, 'ham': 3},\n",
       " 'portfolio': {'spam': 37, 'ham': 92},\n",
       " '871': {'spam': 3, 'ham': 2},\n",
       " 'ny': {'spam': 7, 'ham': 69},\n",
       " 'learn': {'spam': 64, 'ham': 72},\n",
       " 'physical': {'spam': 11, 'ham': 51},\n",
       " 'types': {'spam': 12, 'ham': 36},\n",
       " 'different': {'spam': 29, 'ham': 206},\n",
       " 'low': {'spam': 112, 'ham': 34},\n",
       " 'request': {'spam': 48, 'ham': 316},\n",
       " 'market': {'spam': 97, 'ham': 399},\n",
       " 'long': {'spam': 69, 'ham': 247},\n",
       " 'instruct': {'spam': 2, 'ham': 4},\n",
       " 'high': {'spam': 88, 'ham': 137},\n",
       " 'new': {'spam': 248, 'ham': 653},\n",
       " 'madison': {'spam': 2, 'ham': 3},\n",
       " 'contracts': {'spam': 18, 'ham': 103},\n",
       " 'click': {'spam': 277, 'ham': 117},\n",
       " 'transmission': {'spam': 13, 'ham': 71},\n",
       " 'sell': {'spam': 56, 'ham': 52},\n",
       " 'tools': {'spam': 49, 'ham': 76},\n",
       " 'hurry': {'spam': 7, 'ham': 5},\n",
       " 'future': {'spam': 168, 'ham': 234},\n",
       " 'life': {'spam': 201, 'ham': 51},\n",
       " 'provides': {'spam': 23, 'ham': 71},\n",
       " 'sept': {'spam': 2, 'ham': 14},\n",
       " 'fundamentals': {'spam': 8, 'ham': 27},\n",
       " 'experience': {'spam': 36, 'ham': 175},\n",
       " 'visit': {'spam': 126, 'ham': 285},\n",
       " 'including': {'spam': 88, 'ham': 206},\n",
       " '10128': {'spam': 2, 'ham': 1},\n",
       " 'examples': {'spam': 29, 'ham': 34},\n",
       " 'varied': {'spam': 3, 'ham': 8},\n",
       " 'exchange': {'spam': 39, 'ham': 73},\n",
       " 'syllabus': {'spam': 2, 'ham': 2},\n",
       " 'simulated': {'spam': 2, 'ham': 19},\n",
       " '16': {'spam': 35, 'ham': 360},\n",
       " 'traded': {'spam': 12, 'ham': 27},\n",
       " 'instruments': {'spam': 4, 'ham': 28},\n",
       " 'notices': {'spam': 9, 'ham': 8},\n",
       " 'delegates': {'spam': 2, 'ham': 17},\n",
       " 'electricity': {'spam': 5, 'ham': 142},\n",
       " 'factors': {'spam': 19, 'ham': 66},\n",
       " '1369': {'spam': 2, 'ham': 1},\n",
       " 'volatility': {'spam': 4, 'ham': 114},\n",
       " 'sizes': {'spam': 9, 'ham': 5},\n",
       " 'effect': {'spam': 42, 'ham': 55},\n",
       " 'position': {'spam': 46, 'ham': 237},\n",
       " 'basic': {'spam': 10, 'ham': 54},\n",
       " 'comprehensive': {'spam': 18, 'ham': 24},\n",
       " 'us': {'spam': 239, 'ham': 702},\n",
       " 'opportunities': {'spam': 35, 'ham': 144},\n",
       " 'using': {'spam': 77, 'ham': 243},\n",
       " 'training': {'spam': 23, 'ham': 109},\n",
       " 'discount': {'spam': 17, 'ham': 49},\n",
       " 'identifying': {'spam': 4, 'ham': 20},\n",
       " 'nyc': {'spam': 3, 'ham': 16},\n",
       " 'bilateral': {'spam': 2, 'ham': 4},\n",
       " 'org': {'spam': 57, 'ham': 55},\n",
       " 'mercantile': {'spam': 2, 'ham': 4},\n",
       " 'exercise': {'spam': 14, 'ham': 26},\n",
       " 'emi': {'spam': 2, 'ham': 2},\n",
       " 'medium': {'spam': 9, 'ham': 11},\n",
       " 'overview': {'spam': 7, 'ham': 66},\n",
       " 'ave': {'spam': 25, 'ham': 7},\n",
       " 'experts': {'spam': 11, 'ham': 35},\n",
       " 'assets': {'spam': 20, 'ham': 70},\n",
       " 'book': {'spam': 27, 'ham': 193},\n",
       " '25': {'spam': 69, 'ham': 302},\n",
       " 'invoice': {'spam': 1, 'ham': 29},\n",
       " 'customs': {'spam': 1, 'ham': 3},\n",
       " 'purchased': {'spam': 14, 'ham': 22},\n",
       " 'unsold': {'spam': 2, 'ham': 3},\n",
       " 'returning': {'spam': 12, 'ham': 28},\n",
       " '224': {'spam': 1, 'ham': 4},\n",
       " 'julie': {'spam': 3, 'ham': 60},\n",
       " 'shipped': {'spam': 3, 'ham': 9},\n",
       " 'express': {'spam': 27, 'ham': 45},\n",
       " 'copies': {'spam': 5, 'ham': 102},\n",
       " 'extra': {'spam': 45, 'ham': 36},\n",
       " 'rice': {'spam': 2, 'ham': 158},\n",
       " 'depending': {'spam': 6, 'ham': 28},\n",
       " 'informed': {'spam': 8, 'ham': 68},\n",
       " 'amount': {'spam': 76, 'ham': 91},\n",
       " 'arrive': {'spam': 6, 'ham': 47},\n",
       " 'tomorrow': {'spam': 10, 'ham': 191},\n",
       " 'days': {'spam': 114, 'ham': 274},\n",
       " 'books': {'spam': 13, 'ham': 62},\n",
       " 'digitals': {'spam': 1, 'ham': 5},\n",
       " 'gillian': {'spam': 1, 'ham': 9},\n",
       " 'server': {'spam': 27, 'ham': 26},\n",
       " 'recipients': {'spam': 34, 'ham': 6},\n",
       " 'maubev': {'spam': 2, 'ham': 1},\n",
       " 'failed': {'spam': 35, 'ham': 16},\n",
       " '7': {'spam': 123, 'ham': 314},\n",
       " 'aliceposta': {'spam': 3, 'ham': 1},\n",
       " 'status': {'spam': 24, 'ham': 121},\n",
       " 'authorized': {'spam': 14, 'ham': 24},\n",
       " 'refused': {'spam': 6, 'ham': 5},\n",
       " 'delivery': {'spam': 46, 'ham': 79},\n",
       " 'notification': {'spam': 36, 'ham': 26},\n",
       " 'processed': {'spam': 23, 'ham': 12},\n",
       " 'maxdaffy': {'spam': 2, 'ham': 1},\n",
       " 'assignment': {'spam': 1, 'ham': 28},\n",
       " 'require': {'spam': 30, 'ham': 66},\n",
       " 'friday': {'spam': 6, 'ham': 417},\n",
       " 'initial': {'spam': 18, 'ham': 57},\n",
       " 'coe': {'spam': 1, 'ham': 5},\n",
       " 'pay': {'spam': 45, 'ham': 113},\n",
       " 'anyone': {'spam': 52, 'ham': 106},\n",
       " 'ehronline': {'spam': 1, 'ham': 9},\n",
       " 'system': {'spam': 95, 'ham': 270},\n",
       " 'benefit': {'spam': 31, 'ham': 62},\n",
       " 'change': {'spam': 83, 'ham': 170},\n",
       " 'troubleshooting': {'spam': 1, 'ham': 2},\n",
       " 'general': {'spam': 34, 'ham': 130},\n",
       " '23': {'spam': 18, 'ham': 297},\n",
       " 'letters': {'spam': 5, 'ham': 11},\n",
       " 'approval': {'spam': 17, 'ham': 140},\n",
       " 'get': {'spam': 267, 'ham': 777},\n",
       " 'include': {'spam': 60, 'ham': 224},\n",
       " 'meet': {'spam': 38, 'ham': 300},\n",
       " 'based': {'spam': 72, 'ham': 274},\n",
       " 'attend': {'spam': 3, 'ham': 209},\n",
       " 'sap': {'spam': 1, 'ham': 30},\n",
       " 'july': {'spam': 34, 'ham': 176},\n",
       " 'address': {'spam': 153, 'ham': 294},\n",
       " 'hours': {'spam': 86, 'ham': 85},\n",
       " 'password': {'spam': 8, 'ham': 84},\n",
       " 'processes': {'spam': 9, 'ham': 65},\n",
       " '8': {'spam': 148, 'ham': 408},\n",
       " 'available': {'spam': 132, 'ham': 478},\n",
       " 'interactive': {'spam': 9, 'ham': 19},\n",
       " 'register': {'spam': 20, 'ham': 52},\n",
       " 'personal': {'spam': 62, 'ham': 72},\n",
       " 'version': {'spam': 67, 'ham': 183},\n",
       " 'entry': {'spam': 9, 'ham': 16},\n",
       " 'june': {'spam': 32, 'ham': 163},\n",
       " 'users': {'spam': 29, 'ham': 71},\n",
       " 'direct': {'spam': 21, 'ham': 122},\n",
       " 'hotline': {'spam': 2, 'ham': 11},\n",
       " '4727': {'spam': 1, 'ham': 7},\n",
       " 'functionality': {'spam': 1, 'ham': 18},\n",
       " 'deposit': {'spam': 15, 'ham': 12},\n",
       " 'url': {'spam': 22, 'ham': 41},\n",
       " 'quick': {'spam': 26, 'ham': 82},\n",
       " '345': {'spam': 4, 'ham': 55},\n",
       " 'enables': {'spam': 5, 'ham': 9},\n",
       " 'characters': {'spam': 1, 'ham': 4},\n",
       " 'beginning': {'spam': 6, 'ham': 78},\n",
       " 'numbers': {'spam': 26, 'ham': 117},\n",
       " 'full': {'spam': 97, 'ham': 199},\n",
       " 'live': {'spam': 32, 'ham': 46},\n",
       " '90': {'spam': 50, 'ham': 45},\n",
       " 'select': {'spam': 19, 'ham': 65},\n",
       " 'explorer': {'spam': 4, 'ham': 46},\n",
       " 'allows': {'spam': 23, 'ham': 40},\n",
       " 'connect': {'spam': 6, 'ham': 12},\n",
       " 'criteria': {'spam': 6, 'ham': 24},\n",
       " 'financials': {'spam': 1, 'ham': 5},\n",
       " 'via': {'spam': 32, 'ham': 156},\n",
       " 'logon': {'spam': 1, 'ham': 10},\n",
       " 'reference': {'spam': 25, 'ham': 86},\n",
       " 'modification': {'spam': 3, 'ham': 6},\n",
       " 'tips': {'spam': 7, 'ham': 5},\n",
       " 'manager': {'spam': 33, 'ham': 220},\n",
       " 'wide': {'spam': 19, 'ham': 62},\n",
       " 'joe': {'spam': 4, 'ham': 96},\n",
       " 'perspective': {'spam': 4, 'ham': 67},\n",
       " 'base': {'spam': 16, 'ham': 70},\n",
       " 'european': {'spam': 8, 'ham': 78},\n",
       " 'texan': {'spam': 1, 'ham': 2},\n",
       " 'later': {'spam': 25, 'ham': 164},\n",
       " 'intend': {'spam': 23, 'ham': 26},\n",
       " 'presenting': {'spam': 5, 'ham': 23},\n",
       " 'kind': {'spam': 47, 'ham': 142},\n",
       " 'document': {'spam': 2, 'ham': 107},\n",
       " 'gold': {'spam': 15, 'ham': 29},\n",
       " 'company': {'spam': 187, 'ham': 348},\n",
       " 'becker': {'spam': 1, 'ham': 8},\n",
       " 'organising': {'spam': 1, 'ham': 11},\n",
       " 'today': {'spam': 149, 'ham': 357},\n",
       " 'image': {'spam': 62, 'ham': 25},\n",
       " 'set': {'spam': 47, 'ham': 311},\n",
       " 'sign': {'spam': 21, 'ham': 73},\n",
       " 'mgt': {'spam': 1, 'ham': 4},\n",
       " 'project': {'spam': 26, 'ham': 335},\n",
       " 'additional': {'spam': 63, 'ham': 236},\n",
       " 'stickiness': {'spam': 1, 'ham': 2},\n",
       " 'adding': {'spam': 4, 'ham': 23},\n",
       " 'local': {'spam': 27, 'ham': 47},\n",
       " 'brief': {'spam': 7, 'ham': 78},\n",
       " 'generate': {'spam': 24, 'ham': 24},\n",
       " 'rather': {'spam': 16, 'ham': 88},\n",
       " 'discussion': {'spam': 4, 'ham': 167},\n",
       " 'convey': {'spam': 1, 'ham': 11},\n",
       " 'sven': {'spam': 1, 'ham': 6},\n",
       " 'units': {'spam': 3, 'ham': 72},\n",
       " 'section': {'spam': 42, 'ham': 62},\n",
       " 'educate': {'spam': 1, 'ham': 6},\n",
       " 'channel': {'spam': 2, 'ham': 8},\n",
       " 'making': {'spam': 42, 'ham': 146},\n",
       " 'mind': {'spam': 36, 'ham': 114},\n",
       " 'reasons': {'spam': 16, 'ham': 43},\n",
       " 'commercialise': {'spam': 1, 'ham': 2},\n",
       " 'display': {'spam': 3, 'ham': 17},\n",
       " 'tamarchenko': {'spam': 1, 'ham': 118},\n",
       " 'back': {'spam': 70, 'ham': 439},\n",
       " 'sezgen': {'spam': 1, 'ham': 34},\n",
       " 'sofya': {'spam': 1, 'ham': 10},\n",
       " 'college': {'spam': 10, 'ham': 55},\n",
       " 'encourage': {'spam': 7, 'ham': 40},\n",
       " 'jose': {'spam': 1, 'ham': 39},\n",
       " 'thode': {'spam': 1, 'ham': 3},\n",
       " 'last': {'spam': 64, 'ham': 505},\n",
       " 'cost': {'spam': 78, 'ham': 204},\n",
       " 'interns': {'spam': 1, 'ham': 29},\n",
       " 'rate': {'spam': 57, 'ham': 128},\n",
       " 'kids': {'spam': 6, 'ham': 2},\n",
       " 'remind': {'spam': 1, 'ham': 25},\n",
       " 'deadline': {'spam': 2, 'ham': 69},\n",
       " 'crowd': {'spam': 2, 'ham': 5},\n",
       " 'participation': {'spam': 6, 'ham': 130},\n",
       " 'center': {'spam': 28, 'ham': 154},\n",
       " 'osman': {'spam': 1, 'ham': 40},\n",
       " 'marquez': {'spam': 1, 'ham': 31},\n",
       " 'thomas': {'spam': 8, 'ham': 61},\n",
       " 'united': {'spam': 46, 'ham': 49},\n",
       " 'onto': {'spam': 10, 'ham': 27},\n",
       " 'gibner': {'spam': 1, 'ham': 313},\n",
       " 'intranet': {'spam': 3, 'ham': 29},\n",
       " 'peyton': {'spam': 1, 'ham': 16},\n",
       " 'halliburton': {'spam': 1, 'ham': 22},\n",
       " 'employees': {'spam': 15, 'ham': 117},\n",
       " 'eric': {'spam': 1, 'ham': 65},\n",
       " 'walk': {'spam': 8, 'ham': 24},\n",
       " 'count': {'spam': 9, 'ham': 22},\n",
       " 'rest': {'spam': 23, 'ham': 70},\n",
       " 'kristal': {'spam': 1, 'ham': 9},\n",
       " 'unitedway': {'spam': 1, 'ham': 6},\n",
       " 'summer': {'spam': 11, 'ham': 219},\n",
       " 'listed': {'spam': 40, 'ham': 118},\n",
       " 'raymond': {'spam': 1, 'ham': 82},\n",
       " 'issler': {'spam': 1, 'ham': 59},\n",
       " 'organization': {'spam': 34, 'ham': 117},\n",
       " 'kevin': {'spam': 2, 'ham': 221},\n",
       " 'chance': {'spam': 30, 'ham': 95},\n",
       " 'earlier': {'spam': 3, 'ham': 140},\n",
       " 'aimone': {'spam': 1, 'ham': 8},\n",
       " 'william': {'spam': 3, 'ham': 83},\n",
       " 'samer': {'spam': 1, 'ham': 27},\n",
       " 'maureen': {'spam': 1, 'ham': 104},\n",
       " 'yana': {'spam': 2, 'ham': 10},\n",
       " 'ainsley': {'spam': 1, 'ham': 9},\n",
       " 'moore': {'spam': 2, 'ham': 105},\n",
       " 'log': {'spam': 12, 'ham': 40},\n",
       " 'smith': {'spam': 5, 'ham': 163},\n",
       " 'mandeep': {'spam': 1, 'ham': 3},\n",
       " 'transferred': {'spam': 7, 'ham': 9},\n",
       " 'gaddis': {'spam': 1, 'ham': 7},\n",
       " 'school': {'spam': 15, 'ham': 301},\n",
       " 'believe': {'spam': 56, 'ham': 201},\n",
       " 'takriti': {'spam': 1, 'ham': 13},\n",
       " 'amazingg': {'spam': 2, 'ham': 1},\n",
       " 'dislodge': {'spam': 2, 'ham': 1},\n",
       " 'save': {'spam': 157, 'ham': 34},\n",
       " 'malarial': {'spam': 2, 'ham': 1},\n",
       " 'miiiion': {'spam': 22, 'ham': 1},\n",
       " 'nice': {'spam': 58, 'ham': 74},\n",
       " 'interrupter': {'spam': 2, 'ham': 1},\n",
       " 'countries': {'spam': 41, 'ham': 14},\n",
       " 'v': {'spam': 91, 'ham': 112},\n",
       " 'ensoul': {'spam': 2, 'ham': 1},\n",
       " 'plagiary': {'spam': 2, 'ham': 1},\n",
       " 'impetuous': {'spam': 2, 'ham': 1},\n",
       " 'iity': {'spam': 3, 'ham': 1},\n",
       " 'customers': {'spam': 94, 'ham': 92},\n",
       " 'andmanyother': {'spam': 65, 'ham': 1},\n",
       " '0': {'spam': 133, 'ham': 218},\n",
       " 'locksman': {'spam': 2, 'ham': 1},\n",
       " 'welcome': {'spam': 61, 'ham': 95},\n",
       " 'pharmaceutical': {'spam': 20, 'ham': 2},\n",
       " 'unavoidable': {'spam': 3, 'ham': 6},\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need a class frequency dictionary. This wil store the total number of ham (0) and spam (1) emails are in the dataset. The following line of code will create it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To count the spam and ham emails, you may just sum the respective 1 and 0 values in the training dataset, since the convention is spam = 1 and ham = 0.\n",
    "class_frequency = {'ham': sum(Y_train == 0), 'spam': sum(Y_train == 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ham': 3468, 'spam': 1114}\n"
     ]
    }
   ],
   "source": [
    "print(class_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the proportion of spam in the training dataset, then you may just do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of spam emails in training is: 0.2431\n"
     ]
    }
   ],
   "source": [
    "# The idea is to compute  (amount of spam emails)/(total emails).\n",
    "# Since an email is either spam or ham, total emails = (amount of ham emails) + (amount of spam emails). \n",
    "proportion_spam = class_frequency['spam']/(class_frequency['ham'] + class_frequency['spam'])\n",
    "print(f\"The proportion of spam emails in training is: {proportion_spam:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this matches the value you obtained in some cells below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### Exercise 2\n",
    "\n",
    "In the next exercise, you will implement the function to compute $P(\\text{word} \\mid \\text{spam})$ and $P(\\text{word} \\mid \\text{ham})$. Since the computations are the same for both types of emails, you will create a function to compute $P(\\text{word} \\mid \\text{class})$ where class can be either spam ($1$) or (ham) $0$.\n",
    "\n",
    "Remember that \n",
    "\n",
    "$$P(\\text{word}_i \\mid \\text{class}) = \\frac{\\text{\\# emails in the class (either spam or ham) containing } \\text{word}_i}{\\text{\\# emails in the given class (spam or ham)}}$$\n",
    "\n",
    "**Note that for now you won't worry about whether a word is present or not in the dictionary. This will be handled in later functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def prob_word_given_class(word, cls, word_frequency, class_frequency):\n",
    "    \"\"\"\n",
    "    Calculate the conditional probability of a given word occurring in a specific class.\n",
    "\n",
    "    Parameters:\n",
    "    - word (str): The target word for which the probability is calculated.\n",
    "    - cls (str): The class for which the probability is calculated, it may be 'spam' or 'ham'\n",
    "    - word_frequency (dict): The dictionary containing the words frequency.\n",
    "    - class_frequency (dict): The dictionary containing the class frequency.\n",
    "\n",
    "    Returns:\n",
    "    - float: The conditional probability of the given word occurring in the specified class.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Get the amount of times the word appears with the given class (class is stores in spam variable)\n",
    "    amount_word_and_class = word_frequency[word][cls]\n",
    "    p_word_given_class = amount_word_and_class/class_frequency[cls]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return p_word_given_class\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(lottery | spam) = 0.008367345248374986\n",
      "P(lottery | ham) = 0.0002883506343713956\n",
      "P(schedule | spam) = 0.008976660682226212\n",
      "P(schedule | ham) = 0.10294117647058823\n"
     ]
    }
   ],
   "source": [
    "print(f\"P(lottery | spam) = {prob_word_given_class('lottery', cls = 'spam', word_frequency = word_frequency, class_frequency = class_frequency)+prob_word_given_class('lottery', cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)}\")\n",
    "print(f\"P(lottery | ham) = {prob_word_given_class('lottery', cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)}\")\n",
    "print(f\"P(schedule | spam) = {prob_word_given_class('schedule', cls = 'spam', word_frequency = word_frequency, class_frequency = class_frequency)}\")\n",
    "print(f\"P(schedule | ham) = {prob_word_given_class('schedule', cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ (the results may vary in the last decimal places)\n",
    "\n",
    "```Python\n",
    "P(lottery | spam) = 0.00807899461400359\n",
    "P(lottery | ham) = 0.0002883506343713956\n",
    "P(schedule | spam) = 0.008976660682226212\n",
    "P(schedule | ham) = 0.10294117647058823\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will test your function. Don't worry, you are not being graded yet. This will just ensure your function is working properly. If the unit test fails, you will get feedback so you can review your function before moving on to the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_prob_word_given_class(prob_word_given_class, word_frequency, class_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex03\"></a>\n",
    "### Exercise 3\n",
    "\n",
    "In the next exercise, you will implement the function to compute $P(\\text{email} \\mid \\text{class})$ where class can be either spam (1) or ham (0). You will use the *naive assumption* that \n",
    "\n",
    "$$P(\\text{email} \\mid \\text{class}) = P(\\text{word}_1 \\mid \\text{class}) \\cdot P(\\text{word}_2 \\mid \\text{class}) \\cdots P(\\text{word}_n \\mid \\text{class})$$\n",
    "\n",
    "The idea is to iterate over every word in the email and in each step, update the probability by multiplying it with the value for $P(\\text{word} \\mid \\text{class})$.\n",
    "\n",
    "Remember that, in Python, to update values, instead of using `value = value * update`, you may just use `value *= update`. They perform exactly the same computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def prob_email_given_class(treated_email, cls, word_frequency, class_frequency):\n",
    "    \"\"\"\n",
    "    Calculate the probability of an email being of a certain class (e.g., spam or ham) based on treated email content.\n",
    "\n",
    "    Parameters:\n",
    "    - treated_email (list): A list of treated words in the email.\n",
    "    - cls (str): The class label for the email. It can be either 'spam' or 'ham'\n",
    "    - word_frequency (dict): The dictionary containing the words frequency.\n",
    "    - class_frequency (dict): The dictionary containing the class frequency.\n",
    "\n",
    "    Returns:\n",
    "    - float: The probability of the given email belonging to the specified class.\n",
    "    \"\"\"\n",
    "\n",
    "    # prob starts at 1 because it will be updated by multiplying it with the current P(word | class) in every iteration\n",
    "    prob = 1\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    for word in treated_email:\n",
    "        # Only perform the computation for words that exist in the word frequency dictionary\n",
    "        if word in word_frequency.keys(): \n",
    "            # Update the prob by multiplying it with P(word | class). \n",
    "            # Don't forget to add the word_frequency and class_frequency parameters!\n",
    "            prob *= prob_word_given_class(word,cls,word_frequency,class_frequency)\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: Click here to win a lottery ticket and claim your prize!\n",
      "Email after preprocessing: ['click' 'win' 'lottery' 'ticket' 'claim' 'prize']\n",
      "P(email | spam) = 5.3884806600117164e-11\n",
      "P(email | ham) = 1.2428344868918976e-15\n"
     ]
    }
   ],
   "source": [
    "example_email = \"Click here to win a lottery ticket and claim your prize!\"\n",
    "treated_email = preprocess_text(example_email)\n",
    "prob_spam = prob_email_given_class(treated_email, cls = 'spam', word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "prob_ham = prob_email_given_class(treated_email, cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "print(f\"Email: {example_email}\\nEmail after preprocessing: {treated_email}\\nP(email | spam) = {prob_spam}\\nP(email | ham) = {prob_ham}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### __Expected Output__ (the results may vary in the last decimal places)\n",
    "\n",
    "```Python\n",
    "Email: Click here to win a lottery ticket and claim your prize!\n",
    "Email after preprocessing: ['click' 'win' 'lottery' 'ticket' 'claim' 'prize']\n",
    "P(email | spam) = 5.3884806600117164e-11\n",
    "P(email | ham) = 1.2428344868918976e-15\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will test your function. Don't worry, you are not being graded yet. This will just ensure your function is working properly. If the unit test fails, you will get feedback so you can review your function before moving on to the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_prob_email_given_class(prob_email_given_class, word_frequency, class_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex04\"></a>\n",
    "### Exercise 4\n",
    "\n",
    "In this exercise you will perform both computations below to calculate the probability an email is either spam or ham:\n",
    "\n",
    "- $ P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam}) $\n",
    "\n",
    "- $ P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$\n",
    "\n",
    "The one with the greatest value will be the class your algorithm assigns to that email. Note that the function below includes a parameter that tells the function to return both probabilities rather than the class that was chosen.\n",
    "\n",
    "**Note**: You will notice that the output will be an integer, indicating the respective email class. It would be possible to return spam if the email is predicted as spam and ham if the email is predicted as ham, however, having the model output a number helps further computation, such as metrics to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def naive_bayes(treated_email, word_frequency, class_frequency, return_likelihood = False):    \n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for spam detection.\n",
    "\n",
    "    This function calculates the probability of an email being spam (1) or ham (0)\n",
    "    based on the Naive Bayes algorithm. It uses the conditional probabilities of the\n",
    "    treated_email given spam and ham, as well as the prior probabilities of spam and ham\n",
    "    classes. The final decision is made by comparing the calculated probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - treated_email (list): A preprocessed representation of the input email.\n",
    "    - word_frequency (dict): The dictionary containing the words frequency.\n",
    "    - class_frequency (dict): The dictionary containing the class frequency.\n",
    "    - return_likelihood (bool): If true, it returns the likelihood of both spam and ham.\n",
    "\n",
    "    Returns:\n",
    "    If return_likelihood = False:\n",
    "        - int: 1 if the email is classified as spam, 0 if classified as ham.\n",
    "    If return_likelihood = True:\n",
    "        - tuple: A tuple with the format (spam_likelihood, ham_likelihood)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Compute P(email | spam) with the function you defined just above. \n",
    "    # Don't forget to add the word_frequency and class_frequency parameters!\n",
    "    prob_email_given_spam = prob_email_given_class(treated_email,\"spam\",word_frequency,class_frequency)\n",
    "\n",
    "    # Compute P(email | ham) with the function you defined just above. \n",
    "    # Don't forget to add the word_frequency and class_frequency parameters!\n",
    "    prob_email_given_ham = prob_email_given_class(treated_email, \"ham\", word_frequency, class_frequency)\n",
    "\n",
    "    # Compute P(spam) using the class_frequency dictionary and using the formula #spam emails / #total emails\n",
    "    p_spam = len(str(class_frequency[\"spam\"]))/len(class_frequency)\n",
    "\n",
    "    # Compute P(ham) using the class_frequency dictionary and using the formula #ham emails / #total emails\n",
    "    p_ham = len(str(class_frequency[\"ham\"]))/len(class_frequency)\n",
    "\n",
    "    # Compute the quantity P(spam) * P(email | spam), let's call it spam_likelihood\n",
    "    spam_likelihood = p_spam * prob_email_given_spam\n",
    "\n",
    "    # Compute the quantity P(ham) * P(email | ham), let's call it ham_likelihood\n",
    "    ham_likelihood = p_ham * prob_email_given_ham\n",
    "\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # In case of passing return_likelihood = True, then return the desired tuple\n",
    "    if return_likelihood == True:\n",
    "        return (spam_likelihood, ham_likelihood)\n",
    "    \n",
    "    # Compares both values and choose the class corresponding to the higher value\n",
    "    elif spam_likelihood >= ham_likelihood:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email: Click here to win a lottery ticket and claim your prize!\n",
      "Email after preprocessing: ['click' 'win' 'lottery' 'ticket' 'claim' 'prize']\n",
      "Naive Bayes predicts this email as: 1\n",
      "\n",
      "\n",
      "\n",
      "Email: Our meeting will happen in the main office. Please be there in time.\n",
      "Email after preprocessing: ['meeting' 'happen' 'main' 'office' 'please' 'time']\n",
      "Naive Bayes predicts this email as: 0\n"
     ]
    }
   ],
   "source": [
    "example_email = \"Click here to win a lottery ticket and claim your prize!\"\n",
    "treated_email = preprocess_text(example_email)\n",
    "\n",
    "print(f\"Email: {example_email}\\nEmail after preprocessing: {treated_email}\\nNaive Bayes predicts this email as: {naive_bayes(treated_email, word_frequency, class_frequency)}\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "example_email = \"Our meeting will happen in the main office. Please be there in time.\"\n",
    "treated_email = preprocess_text(example_email)\n",
    "\n",
    "print(f\"Email: {example_email}\\nEmail after preprocessing: {treated_email}\\nNaive Bayes predicts this email as: {naive_bayes(treated_email, word_frequency, class_frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Expected Output__\n",
    "\n",
    "```\n",
    "Email: Click here to win a lottery ticket and claim your prize!\n",
    "Email after preprocessing: ['click' 'win' 'lottery' 'ticket' 'claim' 'prize']\n",
    "Naive Bayes predicts this email as: 1\n",
    "\n",
    "\n",
    "\n",
    "Email: Our meeting will happen in the main office. Please be there in time.\n",
    "Email after preprocessing: ['meeting' 'happen' 'main' 'office' 'please' 'time']\n",
    "Naive Bayes predicts this email as: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block of code will test your function. Don't worry, you are not being graded yet. This will just ensure your function is working properly. If the unit test fails, you will get feedback so you can review your function before moving on to the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "w1_unittest.test_naive_bayes(naive_bayes, word_frequency, class_frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### 4.4 Model performance\n",
    "\n",
    "This section doesn't contain any graded part as it goes beyond what you saw in the lectures. However, we recommend you read it and try to understand what is being done, since measuring a model performance is crucial when building models.\n",
    "\n",
    "In this section you will explore the performance of the model you've just built. Recall you trained you model on 80% of the data, and randomly preserved 20% of your data as test data to test it. The natural question then, is how often the model makes a correct classification when used on your test data. To answer this question, there exists one metric called [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision). This is a measure of how much the model predicts correctly. \n",
    "\n",
    "To compute the accuracy, you must:\n",
    "\n",
    "- Count every spam email that the model correctly classifies as spam (these are called **true positives**)\n",
    "- Count every ham email that the model correctly classifies as ham (these are called **true negatives**)\n",
    "\n",
    "Finally, to get a proportion, you divide the sum of the true positives and true negatives by the total number of observations. If the model is perfect, then the accuracy would be 1, or 100%. The next code block will implement functions to make this calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_positives(Y_true, Y_pred):\n",
    "\n",
    "    # Both Y_true and Y_pred must match in length.\n",
    "    if len(Y_true) != len(Y_pred):\n",
    "        return \"Number of true labels and predict labels must match!\"\n",
    "    n = len(Y_true)\n",
    "    true_positives = 0\n",
    "    # Iterate over the number of elements in the list\n",
    "    for i in range(n):\n",
    "        # Get the true label for the considered email\n",
    "        true_label_i = Y_true[i]\n",
    "        # Get the predicted (model output) for the considered email\n",
    "        predicted_label_i = Y_pred[i]\n",
    "        # Increase the counter by 1 only if true_label_i = 1 and predicted_label_i = 1 (true positives)\n",
    "        if true_label_i == 1 and predicted_label_i == 1:\n",
    "            true_positives += 1\n",
    "    return true_positives\n",
    "        \n",
    "def get_true_negatives(Y_true, Y_pred):\n",
    "    \n",
    "    # Both Y_true and Y_pred must match in length.\n",
    "    if len(Y_true) != len(Y_pred):\n",
    "        return \"Number of true labels and predict labels must match!\"\n",
    "    n = len(Y_true)\n",
    "    true_negatives = 0\n",
    "    # Iterate over the number of elements in the list\n",
    "    for i in range(n):\n",
    "        # Get the true label for the considered email\n",
    "        true_label_i = Y_true[i]\n",
    "        # Get the predicted (model output) for the considered email\n",
    "        predicted_label_i = Y_pred[i]\n",
    "        # Increase the counter by 1 only if true_label_i = 0 and predicted_label_i = 0 (true negatives)\n",
    "        if true_label_i == 0 and predicted_label_i == 0:\n",
    "            true_negatives += 1\n",
    "    return true_negatives\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_test and Y_pred matches in length? Answer: True\n"
     ]
    }
   ],
   "source": [
    "# Let's get the predictions for the test set:\n",
    "\n",
    "# Create an empty list to store the predictions\n",
    "Y_pred = []\n",
    "\n",
    "\n",
    "# Iterate over every email in the test set\n",
    "for email in X_test:\n",
    "    # Perform prediction\n",
    "    prediction = naive_bayes(email, word_frequency, class_frequency)\n",
    "    # Add it to the list \n",
    "    Y_pred.append(prediction)\n",
    "\n",
    "# Checking if both Y_pred and Y_test (these are the true labels) match in length:\n",
    "print(f\"Y_test and Y_pred matches in length? Answer: {len(Y_pred) == len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of true positives is: 250\n",
      "The number of true negatives is: 722\n",
      "Accuracy is: 0.8482\n"
     ]
    }
   ],
   "source": [
    "# Get the number of true positives:\n",
    "true_positives = get_true_positives(Y_test, Y_pred)\n",
    "\n",
    "# Get the number of true negatives:\n",
    "true_negatives = get_true_negatives(Y_test, Y_pred)\n",
    "\n",
    "print(f\"The number of true positives is: {true_positives}\\nThe number of true negatives is: {true_negatives}\")\n",
    "\n",
    "# Compute the accuracy by summing true negatives with true positives and dividing it by the total number of elements in the dataset. \n",
    "# Since both Y_pred and Y_test have the same length, it does not matter which one you use.\n",
    "accuracy = (true_positives + true_negatives)/len(Y_test)\n",
    "\n",
    "print(f\"Accuracy is: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You've developed a solid Naive Bayes model, assuming each word in an email stands alone. Even with that basic approach, the model impressively reaches an accuracy of 84.82%! Well done! Now, in the next code block, go ahead and compose your own email. Now you can experiment with your model in the next code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email is: Please meet me in 2 hours in the main building. I have an important task for you.\n",
      "The model predicts it as ham.\n"
     ]
    }
   ],
   "source": [
    "email = \"Please meet me in 2 hours in the main building. I have an important task for you.\"\n",
    "# email = \"You win a lottery prize! Congratulations! Click here to claim it\"\n",
    "\n",
    "# Preprocess the email\n",
    "treated_email = preprocess_text(email)\n",
    "# Get the prediction, in order to print it nicely, if the output is 1 then the prediction will be written as \"spam\" otherwise \"ham\".\n",
    "prediction = \"spam\" if naive_bayes(treated_email, word_frequency, class_frequency) == 1 else \"ham\"\n",
    "print(f\"The email is: {email}\\nThe model predicts it as {prediction}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Appendix (Section NOT graded)\n",
    "\n",
    "The following sections are not graded but show some interesting extensions of the work you just did. Feel free to submit your work now if you like for grading, but if you want to go deeper you can check out the following sections.\n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "### 5.1 Hidden problem in the Naive Bayes model.\n",
    "\n",
    "A hidden problem in the current model is impacting its performance. Let's delve into the issue by manually performing the Naive Bayes computation on a specific example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The email is:\n",
      "\tfrom the enron india newsdesk - may 5 - 7 newsclips  stinson / vince ,  some news articles . do read the first one , and the second last one .  regards ,  sandeep .  - - - - - - - - - - - - - - - - - - - - - - forwarded by sandeep kohli / enron _ development on  05 / 07 / 2001 09 : 10 am - - - - - - - - - - - - - - - - - - - - - - - - - - -  nikita varma  05 / 07 / 2001 07 : 42 am  to : nikita varma / enron _ development @ enron _ development  cc : ( bcc : sandeep kohli / enron _ development )  subject : from the enron india newsdesk - may 5 - 7 newsclips  the economic times , may 7 , 2001  enron ceo casts vote to save dpc , tina edwin & soma banerjee  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 7 , 2001  maha sore over delay in naming godbole nominee  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the times of india , 7 may , 2001  maharashtra ' unhappy ' with delay in naming godbole nominee  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard , monday , 7 may 2001  reliance allowed to hawk power from patalganga to third parties  arijit de , s ravindran & renni abraham in mumbai  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 7 , 2001  no need of patalganga , bhadravati power : mseb  also appeared in the following newspaper :  the times of india , may 7 , 2001  ' no need of patalganga , bhadravati power '  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard , may 7 , 2001  global bankers ask govt to honour dpc obligations  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard  saturday , 5 may , 2001 ,  ge may pull out as dpc supplier , s ravindran in mumbai  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  hindu businessline , may 5 , 2001  agenda for fresh talks with enron chalked out  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 6 , 2001  http : / / 216 . 34 . 146 . 167 : 8000 / servlet / form  godbole panel meets sans dabhol representation  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 5 , 2001  http : / / 216 . 34 . 146 . 167 : 8000 / servlet / form  ntpc not to buy power from enron : govt  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the times of india , may 7 , 2001  mseb recovers rs 3 . 06 cr arrears in one day  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 7 , 2001  enron ceo casts vote to save dpc , tina edwin & soma banerjee  amul ' s creative directors may have gone back to ad - libbing ' enron or  enr - off ' , but for the big kahuna at the american utility , dabhol is still a  worthwhile project . while the entire enron board had almost decided to call  it quits and proceed with the termination of the $ 2 . 9 - billion power project  at dabhol , the veto exercised by the company chairman kenneth lay has saved  the project \u0001 * - at least for the timebeing . sources said the meeting held on  tuesday at the energy major ' s headquarters in houston could have sounded the  death knell for the only big foreign investment in the indian power sector .  although the future of the project is still pretty uncertain with the lenders  unwilling to continue disbursements unless payment obligations are not  honoured and contractual obligations left unfulfilled , the veto exercised at  this juncture by the chairman of the parent company has come as a big boost  to the indian venture . company sources said : \" we do not know what went on  there but it is true that as of now we are not pulling out . \" with the  engineering procurement and construction contractors ge and equipment  suppliers bechtel too in a cautious mode mode , dpc was finding it even more  difficult to continue the construction of the project as per the schedules .  sources said the stand taken by the rest of the directors on the board would  be in view of the backlash that the company would have to face from its  shareholders if the project actually flopped . enron had similar bitter  experiences in pakistan and it was difficult for the parent company to then  justify such investments to the shareholders .  enron , which had planned a major investments in india ' s infrastructure  sectors such as oil and gas , lng , gas transportation , telecom and broadband  network , has already pulled out most of their personnel from some of these  operations . the company ' s mous with various other majors like indian oil  corporation , too , is in a limbo and the us major ' s stake in the oil and gas  venture is up for grabs . however , even though lay is still hoping to find a  solution to the controversy back home , both dpc and mseb are still to get  down to negotiations .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 7 , 2001  maha sore over delay in naming godbole nominee  the maharashtra government has expressed ' unhappiness ' over the centre ' s  delay in appointing its nominee on the nine - member godbole committee to  renegotiate the power purchase agreement signed between enron - promoted  dabhol power company and state electricity board . \" the committee , which is  to hold discussions with enron officials from houston on may 11 , has only a  month ' s time for renegotiations and with dpc ' s termination notice threat  hanging on our head , time is actually running out . yet there is no official  to represent the union government , \" said a senior state government official .  \" there are media reports that the solicitor - general harish salve would be  appointed , but we are yet to hear anything from their side , \" he said .  the official said the state expected centre to announce its representative  before may 11 , as it would appreciate his crucial presence in the first  session of discussions with enron officials , lenders and gas suppliers .  sources in the mantralaya added the government had also been unhappy over  the centre ' s \" rigid stand \" on not allowing state - owned national thermal power  corporation to buy the excess capacity of dpc ' s total 2 , 184 - mw project .  \" let ntpc and power trading corporation of india come together and sell dpc ' s  surlpus power . we have already mooted this suggestion , but a favourable reply  is yet to come from the union power ministry , \" the official said .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the times of india , 7 may , 2001  maharashtra ' unhappy ' with delay in naming godbole nominee  the maharashtra government has expressed ' unhappiness ' over the centre ' s  delay in appointing its nominee on the nine - member godbole committee to  renegotiate the power purchase agreement ( ppa ) signed between enron promoted  dabhol power company ( dpc ) and the maharashtra state electricity board  ( mseb ) . \" the committee , which is to hold discussions with enron officials  from houston on may 11 , has only a month ' s time for renegotiations and with  dpc ' s termination notice threat hanging on our head , time is actually running  out . yet there is no official to represent the union government , \" a senior  state government official said here on sunday . \" there are media reports that  solicitor general harish salve would be appointed , but we are yet to hear  anything from their side , \" he said .  the official said that the state expected the centre to announce their  representative before may 11 , as it would appreciate his crucial presence in  the first session of discussions with enron officials , lenders and gas  suppliers . sources in the mantralaya added that the government had also been  unhappy over the centre ' s rigid stand on not allowing state - owned national  thermal power corporation ( ntpc ) to buy the excess capacity of dpc ' s total  2 , 184 mw project . \" let ntpc and power trading corporation of india ( ptc ) come  together and sell dpc ' s surlpus power . we have already mooted this  suggestion , but a favourable reply is yet to come from the union power  ministry , \" the official said . the official said that the centre , which was  also responsible for dpc project as it has provided counter guarantee to  enron india , should form a special purpose vehicle for sale of the excess  power to other states . the state government ' s reaction comes in wake of union  power minister suresh prabhu ' s discussion with chief minister vilasrao  deshmukh in delhi few days ago . it was learnt that prabhu told deshmukh  \" there is no question of ntpc buying power from the project since long term  ppas have been signed by ntpc with the buying states \" .  deshmukh had suggested that the central power utility should sell excess  power over and above the 300 - 400 mw needed for the state from the dpc ' s 740  mw phase - i and soon to be commissioned phase - ii of 1 , 444 mw , to other needy  states . considering the high cost of power generated from dpc , which during  the recent months has hovered around rs 7 per unit as against an average cost  of rs 2 . 30 - 2 . 80 a unit from central and state utilities , there would be few  takers for the power from dabhol , the power minister reportedly said .  \" deficit states will buy dpc power only when the cost of power is brought  down , \" he said , adding power ministry would facilitate wheelng of this power  to the buyers .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard , monday , 7 may 2001  reliance allowed to hawk power from patalganga to third parties  arijit de , s ravindran & renni abraham in mumbai  in an unusual departure from normal practice , the maharashtra government has  allowed the reliance group to sell power generated by its 447 - mw patalganga  power project directly to third parties if the maharashtra state  electricity board ( mseb ) does not lift power . the project \u0001 , s power purchase  agreement ( ppa ) has a clause to this effect . the state government \u0001 , s  permission to reliance to hawk power to third parties has to be seen in the  context of its dithering on forwarding to the centre the dabhol power  company \u0001 , s bid for mega power status so that it could sell power to third  parties .  dpc sources told business standard several weeks ago that the company \u0001 , s  application had been pending with the chief minister \u0001 , s office for months .  only now has the state government authorised the godbole committee to  negotiate with dpc on third party sales outside the state . the dpc project  is facing the threat of closure following mseb \u0001 , s inability to buy power from  it , thanks to the board \u0001 , s weak financial position . not only can the  reliance group sell power to third parties within maharashtra , but it can  sell power to utilities outside the state . the ppa does not expressly bar it  from doing so . nor does it specify the category of customers to whom power  can be sold . so , in effect , this suggests that the group could sell power to  industrial and commercial customers in maharashtra and emerge as a rival to  the mseb . the state electricity board derives over 80 per cent of its revenue  from such consumers .  apart from captive power plants , independent power producers in india are  allowed to sell power only to state electricity boards . they can sell power  outside the state only if they qualify for mega power project status . with  its 447 - mw capacity , the patalganga project is not eligible for such status  because mega power rojects are supposed to have a minimum capacity of 1 , 000  mw . speaking on the sidelines of a press conference last week , reliance  industries managing director anil ambani told business standard : a provision  in third parties . ambani was answering a question on whether the mseb \u0001 , s weak  financials and inability to offer escrow cover to the project as emphasised  in the godbole committee report set up to defuse the dabhol crisis would  derail the patalganga project .  the ppa does not have any express restriction as to third party sale outside  the state , a reliance spokesperson confirmed on friday in a faxed response  to questions . a senior mseb official explained that the state government  cleared private power projects some years ago on the basis of the  unrealistically high demand projections contained in a report by a former  mseb official . subsequently , it was realised that the state would be stuck  with excess power . so the reliance group was permitted to sell power to third  parties , he said . the patalganga project along with the ispat group \u0001 , s 1 , 082  mw bhadravati project has been put on hold till the godbole committee submits  its second  report .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 7 , 2001  no need of patalganga , bhadravati power : mseb  the axe seems to have finally fallen on the much - delayed reliance  industries - promoted patalganga and ispat industries ' bhadravati power  projects in maharashtra as the state electricity board has firmly told the  government that \" there is no need of these projects nor their power \" . the  loss - making board has communicated to the government that mseb had \" no  interest \" in patalganga and bhadravati , as it did not have escrow - able  capacity and also that industrial demand for power had slowed down  tremendously in maharashtra , state government sources said here on sunday .  \" in last november itself , mseb had sent an official intimation to the state  government informing its decision in favour of cancellation of the two  projects on several grounds - - including they being unviable and  unaffordable , \" sources said . \" reliance ' s project is no different from that of  dpc ' s . patalganga is also naphtha - based and its ppa is on similar lines . . .  after the enron experience , mseb cannot even dream of another gas - based  power plant in the state , \" a senior mseb official said . he said mseb has  already asked the state government not to provide escrow to both the 447 - mw  patalganga and the 1 , 084 - mw coal - based bhadravati , \" as the us energy major  has almost squeezed us of all over finances \" . when contacted , mseb chairman  vinay bansal said : \" reliance and ispat projects have been put on hold as per  the godbole committee ' s recommendations \" , but expressed inability to give  further details .  currently , bhadravati and patalganga projects have been put on hold as per  godbole committee report , which was set up to review the dpc - mseb ppa and  energy scenario in maharashtra . \" can you go ahead with the project without an  escrow cover ? \" the committee was believed to have asked ispat and reliance  representatives , to which the reply had been negative , sources added .  sources said , as of now , both the projects have not been able to achieve  financial closure as leading financial institutions were not willing to  fund the projects which do not have a \" guaranteed payment \" mechanism from  mseb , which , incidentally it has promised to dpc . \" all the three were cleared  as ' fast - track ' projects , but other than enron , reliance and ispat have been  caught in a quagmire , especially bhadravati , which has been hanging afire  since last nine years , \" they added .  moreover , the mseb official opined that given the current situation , if dpc  calls it quits from india , bhadravati was a safer bet than reliance ' s  patalganga . patalganga ' s power would be mere 50 paise less than that of dpc ' s  that ranges anywhere around approximately rs 4 . 50 per unit to as high as rs  7 , while bhadravati ' s cost could be around rs 3 . 80 to rs 4 per unit , he  informed . mseb ' s installed capacity ended on march 31 , 2001 , wasl 4 , 000 mw and  it has generated 45 , 000 million units with transmission and distribution  losses as high as 39 per cent . ( pti )  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard , may 7 , 2001  global bankers ask govt to honour dpc obligations  tamal bandyopadhyay , surajeet dasgupta & santosh tiwary in mumbai / newdelhi  global arrangers for the dabhol power company have mounted fresh pressure on  the finance ministry to honour the union government \u0001 , s counter - guarantee and  have also set strict conditions for reconsidering the termination of the  power purchase agreement ( ppa ) between the dpc and the maharashtra state  electricity board ( mseb ) . in a related development , the dpc has sent a note  to all lenders saying they would have to bear the consequences of the turn  of events as they have prevented the dpc from serving the ppa termination  notice last month .  the lenders . in their turn , sent a statement - - prepared by the new  york - based legal firm white & case - - defending their stance saying they are  working in the best interest of the project . the lenders are expected to  meet in london over the next fortnight to take stock of the situation . the  deadline for resolving the issues are drawing to a close as 10 days of the  three - week reprieve have passed . at the dpc board meeting in london on may  25 , the lenders had managed to stall the issuance of the termination  notice and got three weeks \u0001 , time for themselves to convince the centre as  well as the maharashtra government to resolve the impasse on the  controversial project . in a letter to finance secretary ajit kumar dated  april 30 , the global arrangers said the government must own up its  responsibility and meet its obligations without further delay . among the  stiff conditions , set by the arrangers , are the demand that the central  government ensure payment of all the pending bills of mseb for december  2000 , january 2001 , february 2001 and march 2001 which remain unpaid  without any protest or reservation by may 7 ( monday ) .  any payment previously made under \u0001 & protest \u0001 8 should be made free and clear of  such protest or any other reservation , and the center should ensure timely  payment of future bills by mseb , they said . meanwhile , sources said that the  finance secretary was expected to meet the international lenders to the  dabhol projects in london stand on the issue . the lenders \u0001 , list of demands  also include asking mseb to take steps required under the existing  contracts to activate the escrow arrangements put in place at the time of  financial close of phase ii of the project by may 7 .  they have demanded that the union government and the maharashtra government  should take all required actions to ensure that no government agency will  take any step to impede the operation of phase i or the construction and  operation of phase ii without due cause . the lenders have also asked them to  ensure that the relevant customs authorities permit import of all goods and  equipment required for the project by may 21 . csfb , anz export finance ,  citi , bank of america and abn amro are the global arrangers for both phase i  as well as phase ii of the project .  the state bank of india , which is also a global arranger for phase ii , did  not sign the letter . \" ten days have passed since the lenders bought three  weeks time from the company delaying its declaration of the termination of  the ppa . since then , nothing has moved at the material level barring mseb ' s  payment of the january bill to the tune of rs 134 crore under protest , \" said  a source among the global arrangers . come forward to meet its obligations  the lenders are planning to meet around mid - may in london and this time  they will be left with no choice but to give the go - ahead to the company to  terminate the ppa unless the finance ministry comes forward to settle the  issue , the source added . the lenders are , however , not ready to take the  blame for any delay in the termination of ppa as implied by the company . the  white & case statement said the lenders are concerned about the fate of the  project and they are exploring all intermediate steps before choosing the  last option - - termination of ppa .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  business standard , saturday , 5 may , 2001  ge may pull out as dpc supplier , s ravindran in mumbai  after us - based bechtel , it is now the turn of general electric to review its  participation as equipment supplier to the controversial 2 , 184 - mw power  project in maharashtra , being set up by the dabhol power company . bechtel is  the epc contractor to the project while ge has supplied the equipment ,  primarily turbines . general electric , like bechtel , also holds 10 per cent in  dabhol power company ( dpc ) . and both bechtel and general electric are worried  about future payments from dpc . sources familiar with the project said that  so far dpc has not defaulted in its payments to general electric . what is  worrying general electric is the possible scenario after june 7 , when about  700 mw power will be commissioned after the second phase trial runs .  dpc and the maharashtra state electricity board ( mseb ) have been locked in a  payments dispute for months . if mseb continues with its stance , dpc in turn  may not be able to pay general electric . in such a situation , ge may walk out  of the project . a final decision will be taken only in june , said sources .  general electric did not respond to a faxed questionnaire sent by business  standard . senior executives at its public relations agency burson  marsteller roger pereira said that only dpc executives were authorised to  speak on the issue . the dpc spokesman declined to comment on the issue .  the first phase of the 740 mw has already been commissioned . after the second  phase of 1 , 444 mwis commissioned by december , 2001 , mseb will have to pay  dpc a minimum of rs 500 crore per month . the escrow account for this was to  have been made operational by april 7 , 2001 . mseb has refused to do this .  earlier , dpc had invoked the political force majeure clause in its contract  with the board . mseb is now arguing that the invocation of this clause has  absolved dpc of all its liabilities .  consequently , it will not operationalise the escrow account . this casts a  further shadow over dpc \u0001 , s ability to pay general electric and bechtel . this  is worrying the lenders to the project as well . the situation has taken a  turn for the worse with dpc practically refusing to re - negotiate the contract  for the second phase with the godbole panel constituted by the maharashtra  government .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  hindu businessline , may 5 , 2001  agenda for fresh talks with enron chalked out  officials of the state government , maharashtra state electricity board ( mseb )  and members of the madhav godbole committee , which recently submitted its  review on the dabhol power project , met here on saturday . the meeting was to  ` ` chart the agenda for renegotiation with enron officials , ' ' a senior mseb  official said . enron officials were scheduled to attend this meeting but  backed out on may 3 . enron had informed the state government that it would  not accept the recommendations of the godbole committee .  ` it is understandable that the company does not find the recommendations  acceptable . but the report is not bound to personal opinions , ' ' the official  said . the next meeting to decide the direction of renegotiation process with  enron is scheduled on may 11 .  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 6 , 2001  godbole panel meets sans dabhol representation  the godbole committee , set up for renegotiating the estranged power purchase  agreement between us energy major enron - promoted dabhol power company and the  state electricity board on saturday held its first internal meeting sans  representatives of the multinational . \" it was an internal meeting to take  stock of the current situation and decide on matter pertaining to the may 11  meet with officials of enron , ge , bechtel and dpc ' s foreign lenders , \" said  state government sources . the meeting , which lasted for almost four hours ,  discussed a strategy to present the committee ' s recommendations made public  last month , they said .  of the nine members of the committee , saturday ' s meeting was attended by five  members - - including godbole , mseb chairman vinay bansal , state energy  secretary v m lal , state finance secretary sudhir shrivastava and kirit  parekh of indira gandhi institute of developmental research . those absent  were hdfc chairman deepak parekh , teri director r k pachauri , former union  energy secretary eas sarma and yet - to - be - appointed representatives of the  centre and central electricity authority . the negotiating committee would  suggest solutions to bring down the exorbitant power tariff , separating of  the liquefied natural gas facility , restructuring of dpc and allowing sale of  excess power through central utilities mainly the national thermal power  corporation , said sources . ( pti  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the economic times , may 5 , 2001  ntpc not to buy power from enron : govt  the centre has ruled out the possibility of national thermal power  corporation buying power generated by us energy giant enron - promoted dabhol  power company . union power minister suresh prabhu is learnt to have stated  this during the meeting with maharashtra chief minister vilasrao deshmukh  last month , convened by the finance minister yashwant sinha to discuss the  enron crisis , said government sources on friday .  prabhu had pointed out that \" there is no question of ntpc buying power from  the project since long - term power purchase agreements have been signed by  ntpc with the buying states \" . maharashtra chief minister vilasrao deshmukh  during the meeting suggested that the central power utility sell the excess  power over and above the 300 - 400 mw needed for the state from the 740 mw  phase - i and soon - to - be - commissioned phase - ii of 1 , 444 - mw , to other needy  states . when contacted , prabhu said the entire controversy over payment  default by maharashtra state electricity board owing to high cost of power  generated by dpc had to be resolved between the state government , and dpc and  centre had very limited role to play . dpc has already slapped one  conciliation notice on the centre and three arbitration notices on the state  government over non - payment of dues amounting to rs 213 - crore - plus interest  rate towards bills due for the months of december 2000 and january 2001 . ( pti )  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  the times of india , may 7 , 2001  mseb recovers rs 3 . 06 cr arrears in one day  in a special day - long drive , nagpur rural zone of maharashtra state  electricity board ( mseb ) has recovered rs 3 . 06 crore as arrears from the  defaulters who had to pay a handsome dividend , and disconnectedl 5 , 000  connections of erring customers last week . according to mseb sources , under  the drive , initiated by chief engineer manohar bapat , with the assistance of  about 5 , 000 employees including engineers , accounts staff and linesmen , a  door - to - door campaign was launched to meet 25 , 000 customers , leading to the  recovery of the dues . power supply to 15 , 000 customers were disconnected on  the spot due to non - payment of arrears in chandrapur , gadchiroli , wardha ,  bhandara , gondia and nagpur districts , it said in a release . the drive met  with stiff resistence from public and the police were called in at many  places to assist the powermen , it added .\n",
      "\n",
      "After preprocessing:\n",
      "\t:['enron' 'india' 'newsdesk' ... 'assist' 'powermen' 'added']\n"
     ]
    }
   ],
   "source": [
    "example_index = 4798\n",
    "example_email = X[example_index]\n",
    "treated_email = preprocess_text(example_email)\n",
    "print(f\"The email is:\\n\\t{example_email}\\n\\nAfter preprocessing:\\n\\t:{treated_email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute $P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})$ and $P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$  in this case. You can do it by passing the argument `return_likelihood = True` in the `naive_bayes` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam_likelihood: 0.0\n",
      "ham_likelihood: 0.0\n"
     ]
    }
   ],
   "source": [
    "spam_likelihood, ham_likelihood = naive_bayes(treated_email, word_frequency = word_frequency, class_frequency = class_frequency, return_likelihood = True)\n",
    "print(f\"spam_likelihood: {spam_likelihood}\\nham_likelihood: {ham_likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is weird, both spam and ham likelihood are $0$! How can it be possible? By the way, by the actual rule, the model classifies as 1 (spam) if $\\text{spam\\_likelihood} \\geq \\text{ham\\_likelihood}$, so this email would be classified as spam. Let's compare the true and predicted labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example email is labeled as: 0\n",
      "Naive bayes model classifies it as: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"The example email is labeled as: {Y[example_index]}\")\n",
    "print(f\"Naive bayes model classifies it as: {naive_bayes(treated_email, word_frequency, class_frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is an email that would be incorrectly sent to the spam folder! However, note that this behavior is peculiar because both likelihoods are $0$. How can it be possible? The answer lies in the math behind it!\n",
    "\n",
    "Consider the main computation for Naive Bayes:\n",
    "\n",
    "$$P(\\text{email} \\mid \\text{spam}) = P(\\text{word}_1 \\mid \\text{spam}) \\cdot P(\\text{word}_2 \\mid \\text{spam}) \\cdots P(\\text{word}_n \\mid \\text{spam})$$\n",
    "\n",
    "It is a product of **every** word in the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example email has: 2657 words in the product.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The example email has: {len(treated_email)} words in the product.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the email you are investigating has $2657$ words! Let's compute the value $P(\\text{word} \\mid \\text{ham})$ for the first 3 words in the email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: enron. P(enron | ham) = 0.5957324106113033\n",
      "Word: india. P(india | ham) = 0.01787773933102653\n",
      "Word: newsdesk. P(newsdesk | ham) = 0.0017301038062283738\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    word = treated_email[i]\n",
    "    p_word_given_ham = prob_word_given_class(word, cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "    print(f\"Word: {word}. P({word} | ham) = {p_word_given_ham}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that they are all probabilities, they are numbers between $0$ and $1$. So, the product being performed is a product of $2657$ numbers between $0$ and $1$. In the best-case scenario, where every word has a probability in the magnitude of $10^{-1}$ (similar to the first word in the example above), the resulting probability would be in the magnitude of $10^{-2657}$a **very small number** that is challenging for any computer to handle with precision. Let's examine Python's limit on floating-point numbers (decimal numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.float_info(max=1.7976931348623157e+308, max_exp=1024, max_10_exp=308, min=2.2250738585072014e-308, min_exp=-1021, min_10_exp=-307, dig=15, mant_dig=53, epsilon=2.220446049250313e-16, radix=2, rounds=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.float_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimum float value has a magnitude of $10^{-308}$, significantly larger than $10^{-2657}$. Consequently, Python interprets the result of the product as $0$ at some point, leading to the loss of all information. In other words, the way your algorithm is currently written, past a certain length, all emails are being classified as spam. Given the nature of this issue, rooted in the very large product required by Naive Bayes, it is crucial to address the problem.\n",
    "\n",
    "#### 5.1.1 The Underflow Problem\n",
    "\n",
    "The challenge you encounter is termed an **underflow problem**, indicating that you are dealing with exceedingly small numbers beyond the computer's precision. In this case, the root cause is the **very large product** involved in Naive Bayes calculations. Fortunately, there is a solution to this issue.\n",
    "\n",
    "Recall that in Naive Bayes, the specific values of probabilities are not critical since the algorithm solely **compares values**. This is why the denominators in the following equations have been disregarded:\n",
    "\n",
    "$$ P(\\text{spam} \\mid \\text{email}) = \\frac{P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})}{P(\\text{email})} $$\n",
    "$$ P(\\text{ham} \\mid \\text{email}) = \\frac{P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})}{P(\\text{email}) } $$\n",
    "\n",
    "Given that the goal is to identify the greater value between the two, and they share the same positive denominator, only the numerators matter. Specifically, the actual values of these two products:\n",
    "\n",
    "$$P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})$$\n",
    "$$P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$$\n",
    "\n",
    "are irrelevant, as long as you can tell which one is larger than the other.\n",
    "\n",
    "If there exists a function that can be applied to these quantities and **preserves the ordering**, then comparing the outputs of these values in such a function will determine the class with the maximum value (although the actual numeric value may differ). \n",
    "\n",
    "Any **strictly increasing function** possesses this property: it preserves the maximum **point**. Therefore, the idea is to find a **increasing function** that aids in handling the large product faced by the Naive Bayes algorithm. Can you think of one? Well, there is one: the $\\log$ function. As you may already know, $\\log$ can transform **products** into **sums**! Since $\\log$ is increasing, it preserves the maximum point. Therefore, you can compare the following quantities:\n",
    "\n",
    "$$\\log \\left(P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam}) \\right)$$\n",
    "$$\\log \\left(P (\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham}) \\right)$$\n",
    "\n",
    "And choose the maximum value among these new quantities. Denoting the class as either spam or ham:\n",
    "\n",
    "$$\\log \\left(P(\\text{class}) \\cdot P(\\text{email} \\mid \\text{class}) \\right) = \\log \\left(P(\\text{class}) \\right) + \\log \\left( P(\\text{email} \\mid \\text{class}) \\right)$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\log \\left( P(\\text{email} \\mid \\text{class}) \\right) = \\log  \\left(P(\\text{word}_1 \\mid \\text{class}) \\cdot P(\\text{word}_2 \\mid \\text{class}) \\cdots P(\\text{word}_n \\mid \\text{class}) \\right) = \\log  \\left(P(\\text{word}_1 \\mid \\text{class}) \\right) + \\log \\left(P(\\text{word}_2 \\mid \\text{class})\\right) + \\cdots + \\log \\left( P(\\text{word}_n \\mid \\text{class}) \\right) $$\n",
    "\n",
    "With this approach, you have transformed a large product into a large summation, a significantly more numerically stable operation. Now, you will improve our functions with this new technique! You need to adjust two functions:\n",
    "\n",
    "- `prob_email_given_class` - replace the probability word product by the sum of the logs\n",
    "- `naive_bayes` - replace the product $P(\\text{class}) \\cdot P(\\text{email} \\mid \\text{class})$ by its respective sum of log.\n",
    "\n",
    "The new functions will be called `log_prob_email_given_class` and `log_naive_bayes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_email_given_class(treated_email, cls, word_frequency, class_frequency):\n",
    "    \"\"\"\n",
    "    Calculate the log probability of an email being of a certain class (e.g., spam or ham) based on treated email content.\n",
    "\n",
    "    Parameters:\n",
    "    - treated_email (list): A list of treated words in the email.\n",
    "    - cls (str): The class label ('spam' or 'ham')\n",
    "    \n",
    "\n",
    "    Returns:\n",
    "    - float: The log probability of the given email belonging to the specified class.\n",
    "    \"\"\"\n",
    "\n",
    "    # prob starts at 0 because it will be updated by summing it with the current log(P(word | class)) in every iteration\n",
    "    prob = 0\n",
    "\n",
    "    for word in treated_email: \n",
    "        # Only perform the computation for words that exist in the word frequency dictionary\n",
    "        if word in word_frequency.keys(): \n",
    "            # Update the prob by summing it with log(P(word | class))\n",
    "            prob += np.log(prob_word_given_class(word, cls,word_frequency, class_frequency))\n",
    "\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word schedule:\n",
      "\tP(schedule | spam) = 0.008976660682226212\n",
      "\tlog(P(schedule | spam)) = -4.713127327493184\n"
     ]
    }
   ],
   "source": [
    "# Consider an email with only one word, so it reduces to compute the value P(word | class) or log(P(word | class)).\n",
    "one_word_email = ['schedule']\n",
    "word = one_word_email[0]\n",
    "prob_spam = prob_email_given_class(one_word_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "log_prob_spam = log_prob_email_given_class(one_word_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "print(f\"For word {word}:\\n\\tP({word} | spam) = {prob_spam}\\n\\tlog(P({word} | spam)) = {log_prob_spam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the $\\text{log}$ was capable of transforming a small number into a negative number with a good magnitude. Furthermore, now the algorithm is performing a sum instead of product.\n",
    "\n",
    "The next code block implements the log_naive_bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_naive_bayes(treated_email, word_frequency, class_frequency, return_likelihood = False):    \n",
    "    \"\"\"\n",
    "    Naive Bayes classifier for spam detection, comparing the log probabilities instead of the actual probabilities.\n",
    "\n",
    "    This function calculates the log probability of an email being spam (1) or ham (0)\n",
    "    based on the Naive Bayes algorithm. It uses the conditional probabilities of the\n",
    "    treated_email given spam and ham, as well as the prior probabilities of spam and ham\n",
    "    classes. The final decision is made by comparing the calculated probabilities.\n",
    "\n",
    "    Parameters:\n",
    "    - treated_email (list): A preprocessed representation of the input email.\n",
    "    - return_likelihood (bool): If true, it returns the log_likelihood of both spam and ham.\n",
    "\n",
    "    Returns:\n",
    "    - int: 1 if the email is classified as spam, 0 if classified as ham.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute P(email | spam) with the new log function\n",
    "    log_prob_email_given_spam = log_prob_email_given_class(treated_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency) \n",
    "\n",
    "    # Compute P(email | ham) with the function you defined just above\n",
    "    log_prob_email_given_ham = log_prob_email_given_class(treated_email, cls = 'ham',word_frequency = word_frequency, class_frequency = class_frequency) \n",
    "\n",
    "    # Compute P(spam) using the class_frequency dictionary and using the formula #spam emails / #total emails\n",
    "    p_spam = class_frequency['spam']/(class_frequency['ham'] + class_frequency['spam']) \n",
    "\n",
    "    # Compute P(ham) using the class_frequency dictionary and using the formula #ham emails / #total emails\n",
    "    p_ham = class_frequency['ham']/(class_frequency['ham'] + class_frequency['spam']) \n",
    "\n",
    "    # Compute the quantity log(P(spam)) + log(P(email | spam)), let's call it log_spam_likelihood\n",
    "    log_spam_likelihood = np.log(p_spam) + log_prob_email_given_spam \n",
    "\n",
    "    # Compute the quantity P(ham) * P(email | ham), let's call it ham_likelihood\n",
    "    log_ham_likelihood = np.log(p_ham) + log_prob_email_given_ham \n",
    "\n",
    "    # In case of passing return_likelihood = True, then return the desired tuple\n",
    "    if return_likelihood == True:\n",
    "        return (log_spam_likelihood, log_ham_likelihood)\n",
    "    \n",
    "    # Compares both values and choose the class corresponding to the higher value. \n",
    "    # As the logarithm is an increasing function, the class with the higher value retains this property.\n",
    "    if log_spam_likelihood >= log_ham_likelihood:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the example from the beginning of the section, you will compute `log_spam_likelihood` and `log_ham_likelihood`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_spam_likelihood: -11532.137516538043\n",
      "log_ham_likelihood: -10281.893202145671\n"
     ]
    }
   ],
   "source": [
    "log_spam_likelihood, log_ham_likelihood = log_naive_bayes(treated_email,word_frequency = word_frequency, class_frequency = class_frequency,return_likelihood = True)\n",
    "print(f\"log_spam_likelihood: {log_spam_likelihood}\\nlog_ham_likelihood: {log_ham_likelihood}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now there are two distinct non-zero numbers! Note the higher one is the `log_ham_likelihood`, therefore the `log_naive_bayes` function will correctly predict this email as ham:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The example email is labeled as: 0\n",
      "Log Naive bayes model classifies it as: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The example email is labeled as: {Y[example_index]}\")\n",
    "print(f\"Log Naive bayes model classifies it as: {log_naive_bayes(treated_email,word_frequency = word_frequency, class_frequency = class_frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this enhanced algorithm, the new accuracy is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of true positives is: 249\n",
      "The number of true negatives is: 888\n",
      "The accuracy is: 0.9921\n"
     ]
    }
   ],
   "source": [
    "# Let's get the predictions for the test set:\n",
    "\n",
    "# Create an empty list to store the predictions\n",
    "Y_pred = []\n",
    "\n",
    "\n",
    "# Iterate over every email in the test set\n",
    "for email in X_test:\n",
    "    # Perform prediction\n",
    "    prediction = log_naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n",
    "    # Add it to the list \n",
    "    Y_pred.append(prediction)\n",
    "\n",
    "# Get the number of true positives:\n",
    "true_positives = get_true_positives(Y_test, Y_pred)\n",
    "\n",
    "# Get the number of true negatives:\n",
    "true_negatives = get_true_negatives(Y_test, Y_pred)\n",
    "\n",
    "print(f\"The number of true positives is: {true_positives}\\nThe number of true negatives is: {true_negatives}\")\n",
    "\n",
    "# Compute the accuracy by summing true negatives with true positives and dividing it by the total number of elements in the dataset. \n",
    "# Since both Y_pred and Y_test have the same length, it does not matter which one you use.\n",
    "accuracy = (true_positives + true_negatives)/len(Y_test)\n",
    "\n",
    "print(f\"The accuracy is: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a **huge** improvement! You've increased the model's accuracy from 84.82% to 99.21% in the test set! An increase of almost 17%. And you haven't touched the dataset, it was an improvement purely in the **math** behind it. Powerful, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"5.2\"></a>\n",
    "### 5.2 Enhancing model performance: Practical implementation with Naive Bayes\n",
    "\n",
    "#### 5.2.1 Introduction\n",
    "\n",
    "In this section you will use both Naive Bayes models (with and without log) you've defined above to solve a problem:\n",
    "\n",
    "You must develop a good spam detection model to run in a specific email software. The dataset you worked with in this assignment is the email base you have from this software. You must build a method to effectively protect users from receiving spam, **but you must avoid sending ham emails to the spam folder** since it might cause a user to lose important emails. On the other hand, it is not that concerning letting pass a couple of spam emails to the inbox folder. \n",
    "\n",
    "#### 5.2.2 Accuracy and its limitations\n",
    "\n",
    "Right now, what is the actual performance of the model you've developed thus far? The accuracy metric you defined above has some limitations, specially in this spam detection problem. You have seen in the beginning of the notebook that the proportion of spam emails in the dataset is 23.88%. So, if you create a rule to send **every email directly to inbox folder** it would correctly classify 76.12% of every email! So this pointless rule has an accuracy of 76.12%. \n",
    "\n",
    "To try to properly answer this question, you can ask yourself two questions:\n",
    "\n",
    "- How many spam emails the algorithm correctly classifies as spam? They are called **true positives**.\n",
    "- How many **ham** emails the algorithm **mistakenly classifies** as spam? They are called **false positives**. **This is the important question you must look closer.**\n",
    "\n",
    "The first question relates to a metric called [*recall*](https://en.wikipedia.org/wiki/Precision_and_recall). To answer the first question, you must count how many spam emails there exist in the dataset and count how many of them are correctly labeled as spam by the model (true positives). This is defined as the recall:\n",
    "\n",
    "$$\\text{recall} = \\frac{\\text{true positives (spam emails correctly labeled as spam)}}{\\text{every spam email}}$$\n",
    "\n",
    "Another way you may see this metric being defined is by considering that a spam email will be either correctly labeled as spam (true positive) or mistakenly labeled as ham (false negative), so \n",
    "\n",
    "$$\\text{recall} =\\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$$\n",
    "\n",
    "You will now make the recall function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the recall for a binary classification task.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_true (array-like): Ground truth labels.\n",
    "    - Y_pred (array-like): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - recall (float): The recall score, which is the ratio of true positives to the total number of actual positives.\n",
    "    \"\"\"\n",
    "    # Get the total number of spam emails. Since they are 1 in the data, it suffices summing all the values in the array Y.\n",
    "    total_number_spams = Y_test.sum()\n",
    "    # Get the true positives\n",
    "    true_positives = get_true_positives(Y_true, Y_pred)\n",
    "    \n",
    "    # Compute the recall\n",
    "    recall = true_positives/total_number_spams\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Naive Bayes model (standard and log versions) to classify every email in the test dataset\n",
    "Y_pred_naive_bayes = []\n",
    "Y_pred_log_naive_bayes = []\n",
    "\n",
    "for email in X_test:\n",
    " prediction = naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n",
    " log_prediction = log_naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n",
    " Y_pred_naive_bayes.append(prediction)\n",
    " Y_pred_log_naive_bayes.append(log_prediction)\n",
    "\n",
    "# Compute the recall for both models\n",
    "recall_naive_bayes = get_recall(Y_test, Y_pred_naive_bayes)\n",
    "recall_log_naive_bayes = get_recall(Y_test, Y_pred_log_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of spam emails the standard Naive Bayes model can correctly classify as spam (recall) is: 0.9843\n",
      "The proportion of spam emails the log Naive Bayes model can correctly classify as spam (recall) is: 0.9803\n"
     ]
    }
   ],
   "source": [
    "print(f\"The proportion of spam emails the standard Naive Bayes model can correctly classify as spam (recall) is: {recall_naive_bayes:.4f}\")\n",
    "print(f\"The proportion of spam emails the log Naive Bayes model can correctly classify as spam (recall) is: {recall_log_naive_bayes:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, both models perform pretty well in **detecting spams**, being able to correctly identify 98% of them! This metric tells us about the model's **sensitivity**. In other words, this metric shows us how effective the model is in detecting a spam email.\n",
    "\n",
    "Now you are left with the second question. It is related to another metric called *[precision](https://en.wikipedia.org/wiki/Precision_and_recall)*. \n",
    "\n",
    "To answer the question you must look at all emails the Naive Bayes models classify as spam and, in that pool, how many are **in fact** spam? This is an important metric to look, because a model that classifies any email as spam is a model that correctly classifies 100% of the spam emails, however it is pointless! Furthermore, you must avoid sending regular emails to the spam folder, otherwise the users may lose important emails.\n",
    "\n",
    "This question is related to what it is called **false positives**. In other words, now you are looking at how many ham emails the algorithm sends to the spam folder. In the next code block, you will build a function to compute the false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false_positives(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the number of false positives instances in binary classification.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_true (list): List of true labels (0 or 1) for each instance.\n",
    "    - Y_pred (list): List of predicted labels (0 or 1) for each instance.\n",
    "\n",
    "    Returns:\n",
    "    - int: Number of false positives, where true label is 0 and predicted label is 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Both Y_true and Y_pred must match in length.\n",
    "    if len(Y_true) != len(Y_pred):\n",
    "        return \"Number of true labels and predict labels must match!\"\n",
    "    n = len(Y_true)\n",
    "\n",
    "    false_positives = 0\n",
    "    # Iterate over the number of elements in the list\n",
    "    for i in range(n):\n",
    "        # Get the true label for the considered email\n",
    "        true_label_i = Y_true[i]\n",
    "        # Get the predicted (model output) for the considered email\n",
    "        predicted_label_i = Y_pred[i]\n",
    "        # Increase the counter by 1 only if true_label_i = 0 and predicted_label_i = 0 (false positive)\n",
    "        if true_label_i == 0 and predicted_label_i == 1:\n",
    "            false_positives += 1\n",
    "    return false_positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the ham emails mistakenly labeled as spam (false positives). Let's use the function get_false_positives you've seen above\n",
    " \n",
    "false_positives_naive_bayes = get_false_positives(Y_test, Y_pred_naive_bayes)\n",
    "false_positives_log_naive_bayes = get_false_positives(Y_test, Y_pred_log_naive_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of false positives in the standard Naive Bayes model: 170\n",
      "Number of false positives in the log Naive Bayes model: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of false positives in the standard Naive Bayes model: {false_positives_naive_bayes}\")\n",
    "print(f\"Number of false positives in the log Naive Bayes model: {false_positives_log_naive_bayes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge improvement! You went from 169 ham emails being mistakenly labeled as spam to only 4! To get a more meaningful number, you can compute the following quantity: \n",
    "\n",
    "- The proportion of actual spam emails (true positives) that exists in the pool of predicted spam emails. Note that the pool of predicted emails consist of **every spam email correctly labeled as spam** (true positives) and **every ham email mistakenly labeled as spam** (false positives).\n",
    "\n",
    "This quantity is called **precision** and it is defined as:\n",
    "\n",
    "$$\\text{precision} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$$\n",
    "\n",
    "This metric tells you how **relevant** the output of your model is. As already discussed, a model that predicts every email as spam can correctly identify every spam email, however its output is irrelevant since it sends every ham email to the spam folder. You will now implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(Y_true, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate precision, a metric for the performance of a classification model,\n",
    "    by computing the ratio of true positives to the sum of true positives and false positives.\n",
    "\n",
    "    Parameters:\n",
    "    - Y_true (list): True labels.\n",
    "    - Y_pred (list): Predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    - precision (float): Precision score.\n",
    "    \"\"\"\n",
    "    # Get the true positives\n",
    "    true_positives = get_true_positives(Y_true, Y_pred)\n",
    "    false_positives = get_false_positives(Y_true, Y_pred)\n",
    "    precision = true_positives/(true_positives + false_positives)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of the standard Naive Bayes model: 0.5952\n",
      "Precision of the log Naive Bayes model: 0.9842\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision of the standard Naive Bayes model: {get_precision(Y_test, Y_pred_naive_bayes):.4f}\")\n",
    "print(f\"Precision of the log Naive Bayes model: {get_precision(Y_test, Y_pred_log_naive_bayes):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first version of the model has a precision of 59.57%. In other words, from 100 emails the model classifies as spam, only around 60 of them are in fact spam. This means that this model would send 40 ham emails to the spam folder, indicating that, even though very sensitive, it is not very reliable. \n",
    "\n",
    "On the other hand, the improved model has a precision of 98.42%! So from 100 emails classified as spam, only around 2 will be actually ham emails. A much more reliable output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have completed the entire assignment and the appendix section! "
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
